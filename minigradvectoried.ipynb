{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, nin, activation=None):\n",
    "        \"\"\"\n",
    "        Initializes a neuron with random weights and bias.\n",
    "\n",
    "        Args:\n",
    "            nin (int): Number of inputs to the neuron.\n",
    "            activation (str): Activation function to use ('sigmoid', 'relu', 'tanh', or 'linear').\n",
    "        \"\"\"\n",
    "        limit = np.sqrt(6 / (nin + 10))  # 10 is num_classes TODO change for robustness later\n",
    "        self.weights = np.random.uniform(-limit, limit, nin)\n",
    "        self.bias = np.random.uniform(-limit, limit)\n",
    "        self.activation = activation\n",
    "\n",
    "        self.input_values = None\n",
    "        self.weighted_sum = 0.0\n",
    "        self.output_value = 0.0\n",
    "\n",
    "        self.weight_gradients = np.zeros(nin)\n",
    "        self.bias_gradient = 0.0\n",
    "        self.input_gradients = np.zeros(nin)\n",
    "\n",
    "    def activate(self, z):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0.0, z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            return z  # linear activation\n",
    "\n",
    "    def activation_derivative(self, activated_output):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return activated_output * (1 - activated_output)\n",
    "        elif self.activation == 'relu':\n",
    "            return 1.0 if activated_output > 0 else 0.0\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - activated_output ** 2\n",
    "        else:\n",
    "            return 1.0  # derivative of linear activation\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the neuron.\n",
    "\n",
    "        Args:\n",
    "            input_values (numpy.ndarray): Inputs to the neuron.\n",
    "\n",
    "        Returns:\n",
    "            float: Output after applying activation function.\n",
    "        \"\"\"\n",
    "        self.input_values = input_values\n",
    "        self.weighted_sum = np.dot(self.weights, input_values) + self.bias\n",
    "        self.output_value = self.activate(self.weighted_sum)\n",
    "        return self.output_value\n",
    "\n",
    "    def backward(self, gradient_from_above):\n",
    "        \"\"\"\n",
    "        Performs the backward pass for the neuron.\n",
    "\n",
    "        Args:\n",
    "            gradient_from_above (float): Gradient from the next layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to pass to the previous layer.\n",
    "        \"\"\"\n",
    "        activation_derivative = self.activation_derivative(self.output_value)\n",
    "        gradient_weighted_sum = gradient_from_above * activation_derivative\n",
    "\n",
    "        # Compute gradients for weights and bias\n",
    "        self.weight_gradients = gradient_weighted_sum * self.input_values\n",
    "        self.bias_gradient = gradient_weighted_sum\n",
    "\n",
    "        # Compute gradients for inputs to propagate to previous layer\n",
    "        self.input_gradients = gradient_weighted_sum * self.weights\n",
    "        return self.input_gradients\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, units, nin, activation=None):\n",
    "        \"\"\"\n",
    "        Initializes a layer with a specified number of neurons.\n",
    "\n",
    "        Args:\n",
    "            units (int): Number of neurons in the layer.\n",
    "            nin (int): Number of inputs each neuron receives.\n",
    "            activation (str): Activation function for the neurons ('sigmoid', 'relu', 'tanh', 'linear').\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.neurons = [Neuron(nin, activation=None if activation == 'softmax' else activation) for _ in range(units)]\n",
    "        self.output_values = None\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the layer.\n",
    "\n",
    "        Args:\n",
    "            input_values (numpy.ndarray): Inputs to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Outputs from the layer.\n",
    "        \"\"\"\n",
    "        # Collect outputs from all neurons in the layer\n",
    "        neuron_outputs = np.array([neuron.forward(input_values) for neuron in self.neurons])\n",
    "\n",
    "        if self.activation == 'softmax':\n",
    "            # Softmax will be handled separately in the MLP class\n",
    "            self.output_values = neuron_outputs\n",
    "        else:\n",
    "            self.output_values = neuron_outputs\n",
    "\n",
    "        return self.output_values\n",
    "\n",
    "    def backward(self, gradients_from_above, clip_value=1.0):\n",
    "        \"\"\"\n",
    "        Performs the backward pass for the layer.\n",
    "\n",
    "        Args:\n",
    "            gradients_from_above (numpy.ndarray): Gradients from the next layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradients to pass to the previous layer.\n",
    "        \"\"\"\n",
    "        gradients_to_previous_layer = np.zeros_like(self.neurons[0].input_gradients)\n",
    "        \n",
    "        for neuron, grad in zip(self.neurons, gradients_from_above):\n",
    "            clipped_grad = np.clip(grad, -clip_value, clip_value)\n",
    "            neuron_input_gradients = neuron.backward(clipped_grad)\n",
    "            gradients_to_previous_layer += neuron_input_gradients\n",
    "    \n",
    "        return gradients_to_previous_layer\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters (weights and biases) of all neurons in the layer.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for the update step.\n",
    "        \"\"\"\n",
    "        for neuron in self.neurons:\n",
    "            # Update weights and biases using gradients\n",
    "            neuron.weights -= learning_rate * neuron.weight_gradients\n",
    "            neuron.bias -= learning_rate * neuron.bias_gradient\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activation_functions):\n",
    "        \"\"\"\n",
    "        Initializes the Multi-Layer Perceptron.\n",
    "        \"\"\"\n",
    "        assert len(layer_sizes) - 1 == len(activation_functions), \"Mismatch between layer sizes and activation functions.\"\n",
    "        self.loss_history = []\n",
    "        self.layers = []\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.layers.append(\n",
    "                Layer(\n",
    "                    units=layer_sizes[i],\n",
    "                    nin=layer_sizes[i - 1],\n",
    "                    activation=activation_functions[i - 1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data  # Returns logits\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        logits = np.nan_to_num(logits, nan=0.0, posinf=500, neginf=-500)\n",
    "        logits = np.clip(logits, -500, 500)\n",
    "        max_logit = np.max(logits)\n",
    "        exps = np.exp(logits - max_logit)\n",
    "        sum_exps = np.sum(exps)\n",
    "        probabilities = exps / sum_exps\n",
    "        return probabilities\n",
    "\n",
    "    def compute_loss(self, predicted_logits, target_outputs):\n",
    "        probabilities = self.softmax(predicted_logits)\n",
    "        epsilon = 1e-15\n",
    "        probabilities = np.clip(probabilities, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(target_outputs * np.log(probabilities))\n",
    "        return loss\n",
    "\n",
    "    def compute_loss_derivative(self, predicted_logits, target_outputs):\n",
    "        probabilities = self.softmax(predicted_logits)\n",
    "        gradients = probabilities - target_outputs\n",
    "        return gradients\n",
    "\n",
    "    def backward(self, loss_gradients):\n",
    "        gradients = loss_gradients\n",
    "        for layer in reversed(self.layers):\n",
    "            gradients = layer.backward(np.clip(gradients, -1.0, 1.0))\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters(learning_rate)\n",
    "\n",
    "    def train(self, training_data, training_targets, epochs, learning_rate, validation_data=None, validation_targets=None, batch_size=64):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(len(training_data))\n",
    "            np.random.shuffle(indices)\n",
    "            training_data = training_data[indices]\n",
    "            training_targets = training_targets[indices]\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            num_batches = int(np.ceil(len(training_data) / batch_size))\n",
    "            \n",
    "            for batch_idx in range(num_batches):\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_inputs = training_data[start:end]\n",
    "                batch_targets = training_targets[start:end]\n",
    "                \n",
    "                batch_loss = 0.0\n",
    "                batch_gradients = np.zeros_like(self.forward(batch_inputs[0]))\n",
    "                \n",
    "                for inputs, target in zip(batch_inputs, batch_targets):\n",
    "                    logits = self.forward(inputs)\n",
    "                    loss = self.compute_loss(logits, target)\n",
    "                    batch_loss += loss\n",
    "                    loss_grad = self.compute_loss_derivative(logits, target)\n",
    "                    batch_gradients += loss_grad\n",
    "                \n",
    "                # Average gradients and loss\n",
    "                batch_gradients /= len(batch_inputs)\n",
    "                batch_loss /= len(batch_inputs)\n",
    "                total_loss += batch_loss\n",
    "                print(f\"batch {batch_idx}/{np.ceil(len(training_data) / batch_size)}, Batch Loss: {batch_loss:.6f}\")\n",
    "\n",
    "                # Backward pass and update\n",
    "                self.backward(batch_gradients)\n",
    "                self.update_parameters(learning_rate)\n",
    "            \n",
    "            average_loss = total_loss / num_batches\n",
    "            self.loss_history.append(average_loss)\n",
    "            \n",
    "            # Print loss every epoch\n",
    "            if epoch % 100 == 0 or epoch == 1:\n",
    "                if validation_data is not None and validation_targets is not None:\n",
    "                    accuracy = self.compute_accuracy(validation_data, validation_targets)\n",
    "                    print(f\"Epoch {epoch}/{epochs}, Average Loss: {average_loss:.6f}, Validation Accuracy: {accuracy:.2%}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch}/{epochs}, Average Loss: {average_loss:.6f}\")\n",
    "    \n",
    "    def compute_accuracy(self, data, targets):\n",
    "        correct = 0\n",
    "        for inputs, target in zip(data, targets):\n",
    "            logits = self.forward(inputs)\n",
    "            probabilities = self.softmax(logits)\n",
    "            predicted_class = np.argmax(probabilities)\n",
    "            true_class = np.argmax(target)\n",
    "            if predicted_class == true_class:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(data)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('/Users/matthew/Downloads/archive/mnist_train.csv')\n",
    "test_data = pd.read_csv('/Users/matthew/Downloads/archive/mnist_test.csv')\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_label shape: (60000, 1)\n",
      "x_train shape: (60000, 784)\n",
      "x_test shape: (10000, 1)\n",
      "x_test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data.iloc[:, [0]]\n",
    "X_train = train_data.iloc[:, 1:]\n",
    "y_test = test_data.iloc[:, [0]]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"x_label shape: {y_train.shape}\")\n",
    "print(f\"x_train shape: {X_train.shape}\")\n",
    "print(f\"x_test shape: {y_test.shape}\")\n",
    "print(f\"x_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "X_test = X_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_train[1]))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0/938.0, Batch Loss: 110.723069\n",
      "batch 1/938.0, Batch Loss: 109.558937\n",
      "batch 2/938.0, Batch Loss: 99.000462\n",
      "batch 3/938.0, Batch Loss: 113.868214\n",
      "batch 4/938.0, Batch Loss: 119.408673\n",
      "batch 5/938.0, Batch Loss: 105.499629\n",
      "batch 6/938.0, Batch Loss: 109.395361\n",
      "batch 7/938.0, Batch Loss: 101.460210\n",
      "batch 8/938.0, Batch Loss: 102.936501\n",
      "batch 9/938.0, Batch Loss: 110.301410\n",
      "batch 10/938.0, Batch Loss: 114.126858\n",
      "batch 11/938.0, Batch Loss: 98.641725\n",
      "batch 12/938.0, Batch Loss: 106.058485\n",
      "batch 13/938.0, Batch Loss: 106.694564\n",
      "batch 14/938.0, Batch Loss: 103.649867\n",
      "batch 15/938.0, Batch Loss: 119.066441\n",
      "batch 16/938.0, Batch Loss: 112.490233\n",
      "batch 17/938.0, Batch Loss: 112.329753\n",
      "batch 18/938.0, Batch Loss: 123.207705\n",
      "batch 19/938.0, Batch Loss: 108.403904\n",
      "batch 20/938.0, Batch Loss: 116.101878\n",
      "batch 21/938.0, Batch Loss: 103.193059\n",
      "batch 22/938.0, Batch Loss: 110.763355\n",
      "batch 23/938.0, Batch Loss: 121.142142\n",
      "batch 24/938.0, Batch Loss: 121.252329\n",
      "batch 25/938.0, Batch Loss: 115.970502\n",
      "batch 26/938.0, Batch Loss: 114.513876\n",
      "batch 27/938.0, Batch Loss: 142.706087\n",
      "batch 28/938.0, Batch Loss: 111.972036\n",
      "batch 29/938.0, Batch Loss: 134.323216\n",
      "batch 30/938.0, Batch Loss: 112.674993\n",
      "batch 31/938.0, Batch Loss: 121.493609\n",
      "batch 32/938.0, Batch Loss: 125.509458\n",
      "batch 33/938.0, Batch Loss: 132.994259\n",
      "batch 34/938.0, Batch Loss: 139.601721\n",
      "batch 35/938.0, Batch Loss: 128.372813\n",
      "batch 36/938.0, Batch Loss: 120.254271\n",
      "batch 37/938.0, Batch Loss: 120.146353\n",
      "batch 38/938.0, Batch Loss: 143.803343\n",
      "batch 39/938.0, Batch Loss: 138.820221\n",
      "batch 40/938.0, Batch Loss: 138.647747\n",
      "batch 41/938.0, Batch Loss: 132.071657\n",
      "batch 42/938.0, Batch Loss: 145.461215\n",
      "batch 43/938.0, Batch Loss: 134.708979\n",
      "batch 44/938.0, Batch Loss: 138.638182\n",
      "batch 45/938.0, Batch Loss: 153.010488\n",
      "batch 46/938.0, Batch Loss: 166.741158\n",
      "batch 47/938.0, Batch Loss: 194.362570\n",
      "batch 48/938.0, Batch Loss: 167.195733\n",
      "batch 49/938.0, Batch Loss: 183.572051\n",
      "batch 50/938.0, Batch Loss: 197.994237\n",
      "batch 51/938.0, Batch Loss: 162.183781\n",
      "batch 52/938.0, Batch Loss: 191.122073\n",
      "batch 53/938.0, Batch Loss: 213.573390\n",
      "batch 54/938.0, Batch Loss: 192.831750\n",
      "batch 55/938.0, Batch Loss: 190.751377\n",
      "batch 56/938.0, Batch Loss: 245.631578\n",
      "batch 57/938.0, Batch Loss: 221.625804\n",
      "batch 58/938.0, Batch Loss: 218.237634\n",
      "batch 59/938.0, Batch Loss: 225.436390\n",
      "batch 60/938.0, Batch Loss: 260.922580\n",
      "batch 61/938.0, Batch Loss: 252.465514\n",
      "batch 62/938.0, Batch Loss: 288.281909\n",
      "batch 63/938.0, Batch Loss: 280.173547\n",
      "batch 64/938.0, Batch Loss: 305.794831\n",
      "batch 65/938.0, Batch Loss: 337.673138\n",
      "batch 66/938.0, Batch Loss: 313.842037\n",
      "batch 67/938.0, Batch Loss: 303.851064\n",
      "batch 68/938.0, Batch Loss: 329.370100\n",
      "batch 69/938.0, Batch Loss: 289.845977\n",
      "batch 70/938.0, Batch Loss: 369.416685\n",
      "batch 71/938.0, Batch Loss: 305.779149\n",
      "batch 72/938.0, Batch Loss: 363.422485\n",
      "batch 73/938.0, Batch Loss: 389.656505\n",
      "batch 74/938.0, Batch Loss: 399.768421\n",
      "batch 75/938.0, Batch Loss: 487.219077\n",
      "batch 76/938.0, Batch Loss: 430.160646\n",
      "batch 77/938.0, Batch Loss: 548.121389\n",
      "batch 78/938.0, Batch Loss: 452.299230\n",
      "batch 79/938.0, Batch Loss: 576.073876\n",
      "batch 80/938.0, Batch Loss: 467.451671\n",
      "batch 81/938.0, Batch Loss: 586.181662\n",
      "batch 82/938.0, Batch Loss: 684.302078\n",
      "batch 83/938.0, Batch Loss: 571.860502\n",
      "batch 84/938.0, Batch Loss: 627.586732\n",
      "batch 85/938.0, Batch Loss: 694.221500\n",
      "batch 86/938.0, Batch Loss: 664.875646\n",
      "batch 87/938.0, Batch Loss: 634.583232\n",
      "batch 88/938.0, Batch Loss: 733.173359\n",
      "batch 89/938.0, Batch Loss: 793.231443\n",
      "batch 90/938.0, Batch Loss: 748.236155\n",
      "batch 91/938.0, Batch Loss: 815.760777\n",
      "batch 92/938.0, Batch Loss: 995.558536\n",
      "batch 93/938.0, Batch Loss: 883.976640\n",
      "batch 94/938.0, Batch Loss: 821.265925\n",
      "batch 95/938.0, Batch Loss: 808.493736\n",
      "batch 96/938.0, Batch Loss: 727.089847\n",
      "batch 97/938.0, Batch Loss: 934.972286\n",
      "batch 98/938.0, Batch Loss: 794.977759\n",
      "batch 99/938.0, Batch Loss: 795.821566\n",
      "batch 100/938.0, Batch Loss: 905.675015\n",
      "batch 101/938.0, Batch Loss: 690.010911\n",
      "batch 102/938.0, Batch Loss: 472.220910\n",
      "batch 103/938.0, Batch Loss: 461.724179\n",
      "batch 104/938.0, Batch Loss: 354.079537\n",
      "batch 105/938.0, Batch Loss: 375.217062\n",
      "batch 106/938.0, Batch Loss: 307.070185\n",
      "batch 107/938.0, Batch Loss: 228.718655\n",
      "batch 108/938.0, Batch Loss: 152.621290\n",
      "batch 109/938.0, Batch Loss: 113.182202\n",
      "batch 110/938.0, Batch Loss: 158.087424\n",
      "batch 111/938.0, Batch Loss: 134.384218\n",
      "batch 112/938.0, Batch Loss: 102.673157\n",
      "batch 113/938.0, Batch Loss: 83.274589\n",
      "batch 114/938.0, Batch Loss: 118.376393\n",
      "batch 115/938.0, Batch Loss: 97.500088\n",
      "batch 116/938.0, Batch Loss: 100.018540\n",
      "batch 117/938.0, Batch Loss: 102.896771\n",
      "batch 118/938.0, Batch Loss: 103.976108\n",
      "batch 119/938.0, Batch Loss: 98.219645\n",
      "batch 120/938.0, Batch Loss: 127.001959\n",
      "batch 121/938.0, Batch Loss: 109.013013\n",
      "batch 122/938.0, Batch Loss: 116.568370\n",
      "batch 123/938.0, Batch Loss: 80.950257\n",
      "batch 124/938.0, Batch Loss: 102.896771\n",
      "batch 125/938.0, Batch Loss: 102.896771\n",
      "batch 126/938.0, Batch Loss: 109.732571\n",
      "batch 127/938.0, Batch Loss: 97.500088\n",
      "batch 128/938.0, Batch Loss: 96.060972\n",
      "batch 129/938.0, Batch Loss: 94.981635\n",
      "batch 130/938.0, Batch Loss: 100.738098\n",
      "batch 131/938.0, Batch Loss: 97.859866\n",
      "batch 132/938.0, Batch Loss: 104.695666\n",
      "batch 133/938.0, Batch Loss: 98.579424\n",
      "batch 134/938.0, Batch Loss: 103.616329\n",
      "batch 135/938.0, Batch Loss: 104.695666\n",
      "batch 136/938.0, Batch Loss: 113.330360\n",
      "batch 137/938.0, Batch Loss: 116.568370\n",
      "batch 138/938.0, Batch Loss: 96.420751\n",
      "batch 139/938.0, Batch Loss: 98.579424\n",
      "batch 140/938.0, Batch Loss: 95.701193\n",
      "batch 141/938.0, Batch Loss: 111.531465\n",
      "batch 142/938.0, Batch Loss: 111.531465\n",
      "batch 143/938.0, Batch Loss: 117.287928\n",
      "batch 144/938.0, Batch Loss: 93.182740\n",
      "batch 145/938.0, Batch Loss: 92.822962\n",
      "batch 146/938.0, Batch Loss: 104.695666\n",
      "batch 147/938.0, Batch Loss: 117.647707\n",
      "batch 148/938.0, Batch Loss: 106.854339\n",
      "batch 149/938.0, Batch Loss: 112.251023\n",
      "batch 150/938.0, Batch Loss: 95.341414\n",
      "batch 151/938.0, Batch Loss: 103.616329\n",
      "batch 152/938.0, Batch Loss: 110.811908\n",
      "batch 153/938.0, Batch Loss: 98.219645\n",
      "batch 154/938.0, Batch Loss: 104.695666\n",
      "batch 155/938.0, Batch Loss: 106.854339\n",
      "batch 156/938.0, Batch Loss: 101.817435\n",
      "batch 157/938.0, Batch Loss: 97.859866\n",
      "batch 158/938.0, Batch Loss: 102.536992\n",
      "batch 159/938.0, Batch Loss: 102.536992\n",
      "batch 160/938.0, Batch Loss: 110.092350\n",
      "batch 161/938.0, Batch Loss: 118.007486\n",
      "batch 162/938.0, Batch Loss: 105.055445\n",
      "batch 163/938.0, Batch Loss: 105.055445\n",
      "batch 164/938.0, Batch Loss: 101.457656\n",
      "batch 165/938.0, Batch Loss: 109.732571\n",
      "batch 166/938.0, Batch Loss: 112.970581\n",
      "batch 167/938.0, Batch Loss: 109.732571\n",
      "batch 168/938.0, Batch Loss: 87.066499\n",
      "batch 169/938.0, Batch Loss: 110.092350\n",
      "batch 170/938.0, Batch Loss: 111.171687\n",
      "batch 171/938.0, Batch Loss: 98.219645\n",
      "batch 172/938.0, Batch Loss: 113.690139\n",
      "batch 173/938.0, Batch Loss: 104.695666\n",
      "batch 174/938.0, Batch Loss: 83.828489\n",
      "batch 175/938.0, Batch Loss: 104.695666\n",
      "batch 176/938.0, Batch Loss: 96.780530\n",
      "batch 177/938.0, Batch Loss: 110.092350\n",
      "batch 178/938.0, Batch Loss: 112.251023\n",
      "batch 179/938.0, Batch Loss: 97.140309\n",
      "batch 180/938.0, Batch Loss: 103.976108\n",
      "batch 181/938.0, Batch Loss: 116.568370\n",
      "batch 182/938.0, Batch Loss: 100.018540\n",
      "batch 183/938.0, Batch Loss: 88.145836\n",
      "batch 184/938.0, Batch Loss: 93.182740\n",
      "batch 185/938.0, Batch Loss: 87.786057\n",
      "batch 186/938.0, Batch Loss: 97.140309\n",
      "batch 187/938.0, Batch Loss: 95.341414\n",
      "batch 188/938.0, Batch Loss: 103.616329\n",
      "batch 189/938.0, Batch Loss: 112.610802\n",
      "batch 190/938.0, Batch Loss: 108.653234\n",
      "batch 191/938.0, Batch Loss: 97.500088\n",
      "batch 192/938.0, Batch Loss: 111.531465\n",
      "batch 193/938.0, Batch Loss: 107.573897\n",
      "batch 194/938.0, Batch Loss: 103.616329\n",
      "batch 195/938.0, Batch Loss: 99.658761\n",
      "batch 196/938.0, Batch Loss: 122.324833\n",
      "batch 197/938.0, Batch Loss: 100.018540\n",
      "batch 198/938.0, Batch Loss: 96.060972\n",
      "batch 199/938.0, Batch Loss: 109.013013\n",
      "batch 200/938.0, Batch Loss: 115.848812\n",
      "batch 201/938.0, Batch Loss: 95.701193\n",
      "batch 202/938.0, Batch Loss: 103.976108\n",
      "batch 203/938.0, Batch Loss: 104.335887\n",
      "batch 204/938.0, Batch Loss: 101.097877\n",
      "batch 205/938.0, Batch Loss: 109.372792\n",
      "batch 206/938.0, Batch Loss: 114.769476\n",
      "batch 207/938.0, Batch Loss: 95.341414\n",
      "batch 208/938.0, Batch Loss: 110.452129\n",
      "batch 209/938.0, Batch Loss: 101.097877\n",
      "batch 210/938.0, Batch Loss: 107.573897\n",
      "batch 211/938.0, Batch Loss: 101.097877\n",
      "batch 212/938.0, Batch Loss: 107.933676\n",
      "batch 213/938.0, Batch Loss: 106.494561\n",
      "batch 214/938.0, Batch Loss: 111.891244\n",
      "batch 215/938.0, Batch Loss: 94.621856\n",
      "batch 216/938.0, Batch Loss: 103.976108\n",
      "batch 217/938.0, Batch Loss: 107.214118\n",
      "batch 218/938.0, Batch Loss: 96.780530\n",
      "batch 219/938.0, Batch Loss: 102.536992\n",
      "batch 220/938.0, Batch Loss: 109.732571\n",
      "batch 221/938.0, Batch Loss: 96.420751\n",
      "batch 222/938.0, Batch Loss: 109.372792\n",
      "batch 223/938.0, Batch Loss: 93.902298\n",
      "batch 224/938.0, Batch Loss: 96.420751\n",
      "batch 225/938.0, Batch Loss: 115.848812\n",
      "batch 226/938.0, Batch Loss: 111.171687\n",
      "batch 227/938.0, Batch Loss: 96.780530\n",
      "batch 228/938.0, Batch Loss: 91.383846\n",
      "batch 229/938.0, Batch Loss: 92.822962\n",
      "batch 230/938.0, Batch Loss: 110.811908\n",
      "batch 231/938.0, Batch Loss: 114.409697\n",
      "batch 232/938.0, Batch Loss: 103.256550\n",
      "batch 233/938.0, Batch Loss: 114.049918\n",
      "batch 234/938.0, Batch Loss: 84.548046\n",
      "batch 235/938.0, Batch Loss: 121.605275\n",
      "batch 236/938.0, Batch Loss: 112.610802\n",
      "batch 237/938.0, Batch Loss: 97.500088\n",
      "batch 238/938.0, Batch Loss: 101.457656\n",
      "batch 239/938.0, Batch Loss: 98.579424\n",
      "batch 240/938.0, Batch Loss: 105.055445\n",
      "batch 241/938.0, Batch Loss: 98.579424\n",
      "batch 242/938.0, Batch Loss: 90.664288\n",
      "batch 243/938.0, Batch Loss: 111.891244\n",
      "batch 244/938.0, Batch Loss: 93.902298\n",
      "batch 245/938.0, Batch Loss: 114.049918\n",
      "batch 246/938.0, Batch Loss: 105.055445\n",
      "batch 247/938.0, Batch Loss: 108.293455\n",
      "batch 248/938.0, Batch Loss: 89.225172\n",
      "batch 249/938.0, Batch Loss: 99.298982\n",
      "batch 250/938.0, Batch Loss: 112.970581\n",
      "batch 251/938.0, Batch Loss: 87.426278\n",
      "batch 252/938.0, Batch Loss: 108.293455\n",
      "batch 253/938.0, Batch Loss: 107.573897\n",
      "batch 254/938.0, Batch Loss: 102.536992\n",
      "batch 255/938.0, Batch Loss: 96.420751\n",
      "batch 256/938.0, Batch Loss: 109.013013\n",
      "batch 257/938.0, Batch Loss: 110.452129\n",
      "batch 258/938.0, Batch Loss: 100.738098\n",
      "batch 259/938.0, Batch Loss: 114.409697\n",
      "batch 260/938.0, Batch Loss: 93.182740\n",
      "batch 261/938.0, Batch Loss: 104.695666\n",
      "batch 262/938.0, Batch Loss: 97.500088\n",
      "batch 263/938.0, Batch Loss: 106.494561\n",
      "batch 264/938.0, Batch Loss: 103.616329\n",
      "batch 265/938.0, Batch Loss: 110.452129\n",
      "batch 266/938.0, Batch Loss: 105.415224\n",
      "batch 267/938.0, Batch Loss: 97.140309\n",
      "batch 268/938.0, Batch Loss: 97.859866\n",
      "batch 269/938.0, Batch Loss: 106.494561\n",
      "batch 270/938.0, Batch Loss: 107.933676\n",
      "batch 271/938.0, Batch Loss: 88.145836\n",
      "batch 272/938.0, Batch Loss: 113.690139\n",
      "batch 273/938.0, Batch Loss: 105.415224\n",
      "batch 274/938.0, Batch Loss: 110.811908\n",
      "batch 275/938.0, Batch Loss: 81.669815\n",
      "batch 276/938.0, Batch Loss: 88.505615\n",
      "batch 277/938.0, Batch Loss: 102.177214\n",
      "batch 278/938.0, Batch Loss: 103.616329\n",
      "batch 279/938.0, Batch Loss: 98.219645\n",
      "batch 280/938.0, Batch Loss: 100.378319\n",
      "batch 281/938.0, Batch Loss: 109.732571\n",
      "batch 282/938.0, Batch Loss: 89.225172\n",
      "batch 283/938.0, Batch Loss: 100.018540\n",
      "batch 284/938.0, Batch Loss: 102.896771\n",
      "batch 285/938.0, Batch Loss: 119.086823\n",
      "batch 286/938.0, Batch Loss: 98.219645\n",
      "batch 287/938.0, Batch Loss: 117.287928\n",
      "batch 288/938.0, Batch Loss: 96.420751\n",
      "batch 289/938.0, Batch Loss: 100.378319\n",
      "batch 290/938.0, Batch Loss: 111.531465\n",
      "batch 291/938.0, Batch Loss: 104.695666\n",
      "batch 292/938.0, Batch Loss: 99.658761\n",
      "batch 293/938.0, Batch Loss: 83.468710\n",
      "batch 294/938.0, Batch Loss: 120.885717\n",
      "batch 295/938.0, Batch Loss: 99.298982\n",
      "batch 296/938.0, Batch Loss: 96.060972\n",
      "batch 297/938.0, Batch Loss: 101.817435\n",
      "batch 298/938.0, Batch Loss: 115.848812\n",
      "batch 299/938.0, Batch Loss: 102.177214\n",
      "batch 300/938.0, Batch Loss: 108.653234\n",
      "batch 301/938.0, Batch Loss: 99.298982\n",
      "batch 302/938.0, Batch Loss: 88.505615\n",
      "batch 303/938.0, Batch Loss: 108.653234\n",
      "batch 304/938.0, Batch Loss: 91.383846\n",
      "batch 305/938.0, Batch Loss: 99.658761\n",
      "batch 306/938.0, Batch Loss: 88.865393\n",
      "batch 307/938.0, Batch Loss: 100.738098\n",
      "batch 308/938.0, Batch Loss: 92.103404\n",
      "batch 309/938.0, Batch Loss: 97.859866\n",
      "batch 310/938.0, Batch Loss: 107.933676\n",
      "batch 311/938.0, Batch Loss: 87.066499\n",
      "batch 312/938.0, Batch Loss: 94.262077\n",
      "batch 313/938.0, Batch Loss: 109.732571\n",
      "batch 314/938.0, Batch Loss: 99.298982\n",
      "batch 315/938.0, Batch Loss: 91.024067\n",
      "batch 316/938.0, Batch Loss: 100.018540\n",
      "batch 317/938.0, Batch Loss: 94.621856\n",
      "batch 318/938.0, Batch Loss: 81.310036\n",
      "batch 319/938.0, Batch Loss: 92.463183\n",
      "batch 320/938.0, Batch Loss: 105.775003\n",
      "batch 321/938.0, Batch Loss: 109.732571\n",
      "batch 322/938.0, Batch Loss: 93.902298\n",
      "batch 323/938.0, Batch Loss: 105.055445\n",
      "batch 324/938.0, Batch Loss: 107.573897\n",
      "batch 325/938.0, Batch Loss: 107.573897\n",
      "batch 326/938.0, Batch Loss: 100.378319\n",
      "batch 327/938.0, Batch Loss: 98.579424\n",
      "batch 328/938.0, Batch Loss: 95.701193\n",
      "batch 329/938.0, Batch Loss: 85.627383\n",
      "batch 330/938.0, Batch Loss: 103.256550\n",
      "batch 331/938.0, Batch Loss: 107.573897\n",
      "batch 332/938.0, Batch Loss: 103.976108\n",
      "batch 333/938.0, Batch Loss: 97.500088\n",
      "batch 334/938.0, Batch Loss: 101.457656\n",
      "batch 335/938.0, Batch Loss: 107.933676\n",
      "batch 336/938.0, Batch Loss: 91.383846\n",
      "batch 337/938.0, Batch Loss: 103.616329\n",
      "batch 338/938.0, Batch Loss: 104.695666\n",
      "batch 339/938.0, Batch Loss: 98.219645\n",
      "batch 340/938.0, Batch Loss: 111.531465\n",
      "batch 341/938.0, Batch Loss: 95.701193\n",
      "batch 342/938.0, Batch Loss: 91.383846\n",
      "batch 343/938.0, Batch Loss: 102.177214\n",
      "batch 344/938.0, Batch Loss: 101.457656\n",
      "batch 345/938.0, Batch Loss: 92.103404\n",
      "batch 346/938.0, Batch Loss: 99.298982\n",
      "batch 347/938.0, Batch Loss: 92.822962\n",
      "batch 348/938.0, Batch Loss: 122.684612\n",
      "batch 349/938.0, Batch Loss: 101.097877\n",
      "batch 350/938.0, Batch Loss: 91.024067\n",
      "batch 351/938.0, Batch Loss: 116.568370\n",
      "batch 352/938.0, Batch Loss: 93.182740\n",
      "batch 353/938.0, Batch Loss: 93.902298\n",
      "batch 354/938.0, Batch Loss: 110.452129\n",
      "batch 355/938.0, Batch Loss: 87.786057\n",
      "batch 356/938.0, Batch Loss: 96.060972\n",
      "batch 357/938.0, Batch Loss: 111.531465\n",
      "batch 358/938.0, Batch Loss: 115.129255\n",
      "batch 359/938.0, Batch Loss: 97.500088\n",
      "batch 360/938.0, Batch Loss: 103.256550\n",
      "batch 361/938.0, Batch Loss: 106.854339\n",
      "batch 362/938.0, Batch Loss: 98.579424\n",
      "batch 363/938.0, Batch Loss: 107.573897\n",
      "batch 364/938.0, Batch Loss: 103.616329\n",
      "batch 365/938.0, Batch Loss: 104.335887\n",
      "batch 366/938.0, Batch Loss: 111.531465\n",
      "batch 367/938.0, Batch Loss: 102.177214\n",
      "batch 368/938.0, Batch Loss: 100.738098\n",
      "batch 369/938.0, Batch Loss: 102.177214\n",
      "batch 370/938.0, Batch Loss: 102.536992\n",
      "batch 371/938.0, Batch Loss: 130.599748\n",
      "batch 372/938.0, Batch Loss: 100.738098\n",
      "batch 373/938.0, Batch Loss: 115.129255\n",
      "batch 374/938.0, Batch Loss: 103.616329\n",
      "batch 375/938.0, Batch Loss: 115.848812\n",
      "batch 376/938.0, Batch Loss: 110.452129\n",
      "batch 377/938.0, Batch Loss: 112.251023\n",
      "batch 378/938.0, Batch Loss: 107.573897\n",
      "batch 379/938.0, Batch Loss: 98.939203\n",
      "batch 380/938.0, Batch Loss: 109.372792\n",
      "batch 381/938.0, Batch Loss: 103.976108\n",
      "batch 382/938.0, Batch Loss: 99.298982\n",
      "batch 383/938.0, Batch Loss: 117.287928\n",
      "batch 384/938.0, Batch Loss: 104.695666\n",
      "batch 385/938.0, Batch Loss: 96.060972\n",
      "batch 386/938.0, Batch Loss: 93.902298\n",
      "batch 387/938.0, Batch Loss: 95.701193\n",
      "batch 388/938.0, Batch Loss: 101.817435\n",
      "batch 389/938.0, Batch Loss: 97.500088\n",
      "batch 390/938.0, Batch Loss: 93.902298\n",
      "batch 391/938.0, Batch Loss: 113.330360\n",
      "batch 392/938.0, Batch Loss: 118.367265\n",
      "batch 393/938.0, Batch Loss: 106.854339\n",
      "batch 394/938.0, Batch Loss: 92.822962\n",
      "batch 395/938.0, Batch Loss: 114.409697\n",
      "batch 396/938.0, Batch Loss: 119.446602\n",
      "batch 397/938.0, Batch Loss: 98.579424\n",
      "batch 398/938.0, Batch Loss: 99.658761\n",
      "batch 399/938.0, Batch Loss: 109.013013\n",
      "batch 400/938.0, Batch Loss: 114.409697\n",
      "batch 401/938.0, Batch Loss: 101.817435\n",
      "batch 402/938.0, Batch Loss: 108.653234\n",
      "batch 403/938.0, Batch Loss: 95.341414\n",
      "batch 404/938.0, Batch Loss: 109.372792\n",
      "batch 405/938.0, Batch Loss: 105.055445\n",
      "batch 406/938.0, Batch Loss: 101.097877\n",
      "batch 407/938.0, Batch Loss: 105.055445\n",
      "batch 408/938.0, Batch Loss: 107.933676\n",
      "batch 409/938.0, Batch Loss: 91.383846\n",
      "batch 410/938.0, Batch Loss: 100.018540\n",
      "batch 411/938.0, Batch Loss: 104.695666\n",
      "batch 412/938.0, Batch Loss: 103.976108\n",
      "batch 413/938.0, Batch Loss: 89.944730\n",
      "batch 414/938.0, Batch Loss: 115.848812\n",
      "batch 415/938.0, Batch Loss: 103.616329\n",
      "batch 416/938.0, Batch Loss: 94.262077\n",
      "batch 417/938.0, Batch Loss: 105.775003\n",
      "batch 418/938.0, Batch Loss: 105.775003\n",
      "batch 419/938.0, Batch Loss: 91.024067\n",
      "batch 420/938.0, Batch Loss: 98.219645\n",
      "batch 421/938.0, Batch Loss: 99.298982\n",
      "batch 422/938.0, Batch Loss: 104.695666\n",
      "batch 423/938.0, Batch Loss: 105.055445\n",
      "batch 424/938.0, Batch Loss: 84.548046\n",
      "batch 425/938.0, Batch Loss: 105.775003\n",
      "batch 426/938.0, Batch Loss: 108.293455\n",
      "batch 427/938.0, Batch Loss: 96.780530\n",
      "batch 428/938.0, Batch Loss: 105.415224\n",
      "batch 429/938.0, Batch Loss: 110.092350\n",
      "batch 430/938.0, Batch Loss: 102.896771\n",
      "batch 431/938.0, Batch Loss: 116.568370\n",
      "batch 432/938.0, Batch Loss: 104.335887\n",
      "batch 433/938.0, Batch Loss: 111.171687\n",
      "batch 434/938.0, Batch Loss: 106.134782\n",
      "batch 435/938.0, Batch Loss: 99.298982\n",
      "batch 436/938.0, Batch Loss: 106.134782\n",
      "batch 437/938.0, Batch Loss: 97.500088\n",
      "batch 438/938.0, Batch Loss: 103.976108\n",
      "batch 439/938.0, Batch Loss: 98.219645\n",
      "batch 440/938.0, Batch Loss: 110.452129\n",
      "batch 441/938.0, Batch Loss: 100.018540\n",
      "batch 442/938.0, Batch Loss: 107.214118\n",
      "batch 443/938.0, Batch Loss: 106.134782\n",
      "batch 444/938.0, Batch Loss: 94.262077\n",
      "batch 445/938.0, Batch Loss: 105.415224\n",
      "batch 446/938.0, Batch Loss: 100.738098\n",
      "batch 447/938.0, Batch Loss: 105.415224\n",
      "batch 448/938.0, Batch Loss: 114.409697\n",
      "batch 449/938.0, Batch Loss: 114.409697\n",
      "batch 450/938.0, Batch Loss: 101.097877\n",
      "batch 451/938.0, Batch Loss: 95.341414\n",
      "batch 452/938.0, Batch Loss: 108.293455\n",
      "batch 453/938.0, Batch Loss: 91.743625\n",
      "batch 454/938.0, Batch Loss: 85.267604\n",
      "batch 455/938.0, Batch Loss: 92.822962\n",
      "batch 456/938.0, Batch Loss: 96.420751\n",
      "batch 457/938.0, Batch Loss: 95.341414\n",
      "batch 458/938.0, Batch Loss: 96.060972\n",
      "batch 459/938.0, Batch Loss: 96.780530\n",
      "batch 460/938.0, Batch Loss: 103.976108\n",
      "batch 461/938.0, Batch Loss: 107.214118\n",
      "batch 462/938.0, Batch Loss: 93.182740\n",
      "batch 463/938.0, Batch Loss: 111.171687\n",
      "batch 464/938.0, Batch Loss: 105.415224\n",
      "batch 465/938.0, Batch Loss: 102.896771\n",
      "batch 466/938.0, Batch Loss: 116.568370\n",
      "batch 467/938.0, Batch Loss: 99.298982\n",
      "batch 468/938.0, Batch Loss: 103.256550\n",
      "batch 469/938.0, Batch Loss: 95.341414\n",
      "batch 470/938.0, Batch Loss: 96.420751\n",
      "batch 471/938.0, Batch Loss: 100.018540\n",
      "batch 472/938.0, Batch Loss: 113.330360\n",
      "batch 473/938.0, Batch Loss: 101.817435\n",
      "batch 474/938.0, Batch Loss: 91.383846\n",
      "batch 475/938.0, Batch Loss: 89.584951\n",
      "batch 476/938.0, Batch Loss: 89.944730\n",
      "batch 477/938.0, Batch Loss: 106.134782\n",
      "batch 478/938.0, Batch Loss: 99.298982\n",
      "batch 479/938.0, Batch Loss: 104.335887\n",
      "batch 480/938.0, Batch Loss: 101.817435\n",
      "batch 481/938.0, Batch Loss: 107.573897\n",
      "batch 482/938.0, Batch Loss: 94.262077\n",
      "batch 483/938.0, Batch Loss: 103.976108\n",
      "batch 484/938.0, Batch Loss: 113.330360\n",
      "batch 485/938.0, Batch Loss: 104.335887\n",
      "batch 486/938.0, Batch Loss: 100.018540\n",
      "batch 487/938.0, Batch Loss: 95.701193\n",
      "batch 488/938.0, Batch Loss: 89.584951\n",
      "batch 489/938.0, Batch Loss: 101.457656\n",
      "batch 490/938.0, Batch Loss: 101.097877\n",
      "batch 491/938.0, Batch Loss: 99.658761\n",
      "batch 492/938.0, Batch Loss: 102.896771\n",
      "batch 493/938.0, Batch Loss: 106.494561\n",
      "batch 494/938.0, Batch Loss: 105.055445\n",
      "batch 495/938.0, Batch Loss: 95.341414\n",
      "batch 496/938.0, Batch Loss: 109.013013\n",
      "batch 497/938.0, Batch Loss: 89.584951\n",
      "batch 498/938.0, Batch Loss: 105.775003\n",
      "batch 499/938.0, Batch Loss: 98.939203\n",
      "batch 500/938.0, Batch Loss: 114.769476\n",
      "batch 501/938.0, Batch Loss: 116.208591\n",
      "batch 502/938.0, Batch Loss: 91.743625\n",
      "batch 503/938.0, Batch Loss: 99.658761\n",
      "batch 504/938.0, Batch Loss: 101.097877\n",
      "batch 505/938.0, Batch Loss: 112.251023\n",
      "batch 506/938.0, Batch Loss: 103.256550\n",
      "batch 507/938.0, Batch Loss: 106.494561\n",
      "batch 508/938.0, Batch Loss: 92.822962\n",
      "batch 509/938.0, Batch Loss: 114.769476\n",
      "batch 510/938.0, Batch Loss: 104.695666\n",
      "batch 511/938.0, Batch Loss: 101.097877\n",
      "batch 512/938.0, Batch Loss: 113.330360\n",
      "batch 513/938.0, Batch Loss: 110.092350\n",
      "batch 514/938.0, Batch Loss: 101.097877\n",
      "batch 515/938.0, Batch Loss: 97.500088\n",
      "batch 516/938.0, Batch Loss: 97.140309\n",
      "batch 517/938.0, Batch Loss: 114.769476\n",
      "batch 518/938.0, Batch Loss: 93.542519\n",
      "batch 519/938.0, Batch Loss: 94.262077\n",
      "batch 520/938.0, Batch Loss: 101.097877\n",
      "batch 521/938.0, Batch Loss: 109.372792\n",
      "batch 522/938.0, Batch Loss: 107.573897\n",
      "batch 523/938.0, Batch Loss: 98.939203\n",
      "batch 524/938.0, Batch Loss: 116.568370\n",
      "batch 525/938.0, Batch Loss: 93.182740\n",
      "batch 526/938.0, Batch Loss: 95.701193\n",
      "batch 527/938.0, Batch Loss: 96.060972\n",
      "batch 528/938.0, Batch Loss: 101.097877\n",
      "batch 529/938.0, Batch Loss: 105.415224\n",
      "batch 530/938.0, Batch Loss: 99.298982\n",
      "batch 531/938.0, Batch Loss: 103.616329\n",
      "batch 532/938.0, Batch Loss: 98.219645\n",
      "batch 533/938.0, Batch Loss: 98.219645\n",
      "batch 534/938.0, Batch Loss: 98.939203\n",
      "batch 535/938.0, Batch Loss: 84.188267\n",
      "batch 536/938.0, Batch Loss: 112.251023\n",
      "batch 537/938.0, Batch Loss: 102.896771\n",
      "batch 538/938.0, Batch Loss: 110.452129\n",
      "batch 539/938.0, Batch Loss: 114.409697\n",
      "batch 540/938.0, Batch Loss: 113.330360\n",
      "batch 541/938.0, Batch Loss: 100.018540\n",
      "batch 542/938.0, Batch Loss: 97.500088\n",
      "batch 543/938.0, Batch Loss: 112.970581\n",
      "batch 544/938.0, Batch Loss: 94.262077\n",
      "batch 545/938.0, Batch Loss: 102.896771\n",
      "batch 546/938.0, Batch Loss: 93.902298\n",
      "batch 547/938.0, Batch Loss: 107.573897\n",
      "batch 548/938.0, Batch Loss: 111.891244\n",
      "batch 549/938.0, Batch Loss: 76.992689\n",
      "batch 550/938.0, Batch Loss: 115.848812\n",
      "batch 551/938.0, Batch Loss: 122.324833\n",
      "batch 552/938.0, Batch Loss: 112.610802\n",
      "batch 553/938.0, Batch Loss: 89.584951\n",
      "batch 554/938.0, Batch Loss: 114.769476\n",
      "batch 555/938.0, Batch Loss: 86.346941\n",
      "batch 556/938.0, Batch Loss: 99.298982\n",
      "batch 557/938.0, Batch Loss: 106.134782\n",
      "batch 558/938.0, Batch Loss: 82.389373\n",
      "batch 559/938.0, Batch Loss: 108.293455\n",
      "batch 560/938.0, Batch Loss: 102.896771\n",
      "batch 561/938.0, Batch Loss: 109.732571\n",
      "batch 562/938.0, Batch Loss: 102.536992\n",
      "batch 563/938.0, Batch Loss: 106.854339\n",
      "batch 564/938.0, Batch Loss: 106.134782\n",
      "batch 565/938.0, Batch Loss: 98.219645\n",
      "batch 566/938.0, Batch Loss: 97.859866\n",
      "batch 567/938.0, Batch Loss: 96.060972\n",
      "batch 568/938.0, Batch Loss: 119.806381\n",
      "batch 569/938.0, Batch Loss: 80.950257\n",
      "batch 570/938.0, Batch Loss: 126.282401\n",
      "batch 571/938.0, Batch Loss: 96.420751\n",
      "batch 572/938.0, Batch Loss: 96.060972\n",
      "batch 573/938.0, Batch Loss: 105.415224\n",
      "batch 574/938.0, Batch Loss: 98.579424\n",
      "batch 575/938.0, Batch Loss: 99.298982\n",
      "batch 576/938.0, Batch Loss: 93.542519\n",
      "batch 577/938.0, Batch Loss: 102.896771\n",
      "batch 578/938.0, Batch Loss: 101.457656\n",
      "batch 579/938.0, Batch Loss: 101.097877\n",
      "batch 580/938.0, Batch Loss: 111.531465\n",
      "batch 581/938.0, Batch Loss: 109.013013\n",
      "batch 582/938.0, Batch Loss: 100.378319\n",
      "batch 583/938.0, Batch Loss: 102.536992\n",
      "batch 584/938.0, Batch Loss: 101.457656\n",
      "batch 585/938.0, Batch Loss: 101.817435\n",
      "batch 586/938.0, Batch Loss: 105.415224\n",
      "batch 587/938.0, Batch Loss: 96.060972\n",
      "batch 588/938.0, Batch Loss: 108.293455\n",
      "batch 589/938.0, Batch Loss: 100.018540\n",
      "batch 590/938.0, Batch Loss: 103.616329\n",
      "batch 591/938.0, Batch Loss: 94.262077\n",
      "batch 592/938.0, Batch Loss: 94.981635\n",
      "batch 593/938.0, Batch Loss: 90.304509\n",
      "batch 594/938.0, Batch Loss: 100.378319\n",
      "batch 595/938.0, Batch Loss: 112.970581\n",
      "batch 596/938.0, Batch Loss: 117.287928\n",
      "batch 597/938.0, Batch Loss: 100.738098\n",
      "batch 598/938.0, Batch Loss: 114.049918\n",
      "batch 599/938.0, Batch Loss: 98.579424\n",
      "batch 600/938.0, Batch Loss: 109.372792\n",
      "batch 601/938.0, Batch Loss: 99.658761\n",
      "batch 602/938.0, Batch Loss: 114.049918\n",
      "batch 603/938.0, Batch Loss: 92.463183\n",
      "batch 604/938.0, Batch Loss: 99.298982\n",
      "batch 605/938.0, Batch Loss: 115.848812\n",
      "batch 606/938.0, Batch Loss: 102.536992\n",
      "batch 607/938.0, Batch Loss: 106.134782\n",
      "batch 608/938.0, Batch Loss: 113.690139\n",
      "batch 609/938.0, Batch Loss: 100.378319\n",
      "batch 610/938.0, Batch Loss: 93.902298\n",
      "batch 611/938.0, Batch Loss: 101.457656\n",
      "batch 612/938.0, Batch Loss: 99.658761\n",
      "batch 613/938.0, Batch Loss: 115.129255\n",
      "batch 614/938.0, Batch Loss: 105.415224\n",
      "batch 615/938.0, Batch Loss: 101.097877\n",
      "batch 616/938.0, Batch Loss: 93.182740\n",
      "batch 617/938.0, Batch Loss: 99.658761\n",
      "batch 618/938.0, Batch Loss: 116.568370\n",
      "batch 619/938.0, Batch Loss: 105.415224\n",
      "batch 620/938.0, Batch Loss: 101.817435\n",
      "batch 621/938.0, Batch Loss: 115.129255\n",
      "batch 622/938.0, Batch Loss: 98.939203\n",
      "batch 623/938.0, Batch Loss: 95.341414\n",
      "batch 624/938.0, Batch Loss: 92.103404\n",
      "batch 625/938.0, Batch Loss: 102.536992\n",
      "batch 626/938.0, Batch Loss: 101.817435\n",
      "batch 627/938.0, Batch Loss: 105.415224\n",
      "batch 628/938.0, Batch Loss: 107.933676\n",
      "batch 629/938.0, Batch Loss: 94.262077\n",
      "batch 630/938.0, Batch Loss: 100.738098\n",
      "batch 631/938.0, Batch Loss: 109.013013\n",
      "batch 632/938.0, Batch Loss: 91.024067\n",
      "batch 633/938.0, Batch Loss: 100.738098\n",
      "batch 634/938.0, Batch Loss: 103.256550\n",
      "batch 635/938.0, Batch Loss: 99.658761\n",
      "batch 636/938.0, Batch Loss: 91.024067\n",
      "batch 637/938.0, Batch Loss: 85.627383\n",
      "batch 638/938.0, Batch Loss: 111.891244\n",
      "batch 639/938.0, Batch Loss: 109.013013\n",
      "batch 640/938.0, Batch Loss: 108.653234\n",
      "batch 641/938.0, Batch Loss: 96.420751\n",
      "batch 642/938.0, Batch Loss: 103.616329\n",
      "batch 643/938.0, Batch Loss: 94.621856\n",
      "batch 644/938.0, Batch Loss: 119.806381\n",
      "batch 645/938.0, Batch Loss: 99.298982\n",
      "batch 646/938.0, Batch Loss: 101.097877\n",
      "batch 647/938.0, Batch Loss: 104.695666\n",
      "batch 648/938.0, Batch Loss: 101.457656\n",
      "batch 649/938.0, Batch Loss: 111.891244\n",
      "batch 650/938.0, Batch Loss: 99.658761\n",
      "batch 651/938.0, Batch Loss: 91.383846\n",
      "batch 652/938.0, Batch Loss: 92.103404\n",
      "batch 653/938.0, Batch Loss: 103.256550\n",
      "batch 654/938.0, Batch Loss: 109.732571\n",
      "batch 655/938.0, Batch Loss: 121.245496\n",
      "batch 656/938.0, Batch Loss: 102.536992\n",
      "batch 657/938.0, Batch Loss: 108.293455\n",
      "batch 658/938.0, Batch Loss: 103.256550\n",
      "batch 659/938.0, Batch Loss: 92.103404\n",
      "batch 660/938.0, Batch Loss: 99.658761\n",
      "batch 661/938.0, Batch Loss: 115.489034\n",
      "batch 662/938.0, Batch Loss: 93.182740\n",
      "batch 663/938.0, Batch Loss: 110.811908\n",
      "batch 664/938.0, Batch Loss: 117.647707\n",
      "batch 665/938.0, Batch Loss: 95.701193\n",
      "batch 666/938.0, Batch Loss: 98.939203\n",
      "batch 667/938.0, Batch Loss: 97.500088\n",
      "batch 668/938.0, Batch Loss: 109.372792\n",
      "batch 669/938.0, Batch Loss: 117.647707\n",
      "batch 670/938.0, Batch Loss: 99.658761\n",
      "batch 671/938.0, Batch Loss: 109.372792\n",
      "batch 672/938.0, Batch Loss: 91.743625\n",
      "batch 673/938.0, Batch Loss: 111.171687\n",
      "batch 674/938.0, Batch Loss: 106.494561\n",
      "batch 675/938.0, Batch Loss: 94.981635\n",
      "batch 676/938.0, Batch Loss: 85.987162\n",
      "batch 677/938.0, Batch Loss: 99.658761\n",
      "batch 678/938.0, Batch Loss: 90.664288\n",
      "batch 679/938.0, Batch Loss: 106.854339\n",
      "batch 680/938.0, Batch Loss: 105.415224\n",
      "batch 681/938.0, Batch Loss: 108.293455\n",
      "batch 682/938.0, Batch Loss: 112.251023\n",
      "batch 683/938.0, Batch Loss: 98.579424\n",
      "batch 684/938.0, Batch Loss: 93.182740\n",
      "batch 685/938.0, Batch Loss: 103.256550\n",
      "batch 686/938.0, Batch Loss: 99.658761\n",
      "batch 687/938.0, Batch Loss: 99.298982\n",
      "batch 688/938.0, Batch Loss: 95.701193\n",
      "batch 689/938.0, Batch Loss: 91.383846\n",
      "batch 690/938.0, Batch Loss: 85.267604\n",
      "batch 691/938.0, Batch Loss: 104.695666\n",
      "batch 692/938.0, Batch Loss: 103.256550\n",
      "batch 693/938.0, Batch Loss: 109.372792\n",
      "batch 694/938.0, Batch Loss: 105.415224\n",
      "batch 695/938.0, Batch Loss: 104.695666\n",
      "batch 696/938.0, Batch Loss: 92.822962\n",
      "batch 697/938.0, Batch Loss: 97.859866\n",
      "batch 698/938.0, Batch Loss: 105.055445\n",
      "batch 699/938.0, Batch Loss: 109.013013\n",
      "batch 700/938.0, Batch Loss: 102.896771\n",
      "batch 701/938.0, Batch Loss: 96.060972\n",
      "batch 702/938.0, Batch Loss: 100.738098\n",
      "batch 703/938.0, Batch Loss: 94.981635\n",
      "batch 704/938.0, Batch Loss: 104.695666\n",
      "batch 705/938.0, Batch Loss: 104.695666\n",
      "batch 706/938.0, Batch Loss: 96.060972\n",
      "batch 707/938.0, Batch Loss: 98.939203\n",
      "batch 708/938.0, Batch Loss: 106.494561\n",
      "batch 709/938.0, Batch Loss: 101.097877\n",
      "batch 710/938.0, Batch Loss: 105.055445\n",
      "batch 711/938.0, Batch Loss: 98.579424\n",
      "batch 712/938.0, Batch Loss: 84.548046\n",
      "batch 713/938.0, Batch Loss: 97.140309\n",
      "batch 714/938.0, Batch Loss: 99.298982\n",
      "batch 715/938.0, Batch Loss: 87.426278\n",
      "batch 716/938.0, Batch Loss: 104.335887\n",
      "batch 717/938.0, Batch Loss: 113.330360\n",
      "batch 718/938.0, Batch Loss: 102.896771\n",
      "batch 719/938.0, Batch Loss: 104.335887\n",
      "batch 720/938.0, Batch Loss: 103.616329\n",
      "batch 721/938.0, Batch Loss: 102.177214\n",
      "batch 722/938.0, Batch Loss: 101.817435\n",
      "batch 723/938.0, Batch Loss: 98.579424\n",
      "batch 724/938.0, Batch Loss: 97.500088\n",
      "batch 725/938.0, Batch Loss: 97.140309\n",
      "batch 726/938.0, Batch Loss: 110.811908\n",
      "batch 727/938.0, Batch Loss: 103.256550\n",
      "batch 728/938.0, Batch Loss: 106.854339\n",
      "batch 729/938.0, Batch Loss: 98.579424\n",
      "batch 730/938.0, Batch Loss: 103.256550\n",
      "batch 731/938.0, Batch Loss: 101.097877\n",
      "batch 732/938.0, Batch Loss: 110.092350\n",
      "batch 733/938.0, Batch Loss: 94.262077\n",
      "batch 734/938.0, Batch Loss: 113.330360\n",
      "batch 735/938.0, Batch Loss: 91.024067\n",
      "batch 736/938.0, Batch Loss: 96.420751\n",
      "batch 737/938.0, Batch Loss: 109.372792\n",
      "batch 738/938.0, Batch Loss: 106.854339\n",
      "batch 739/938.0, Batch Loss: 88.865393\n",
      "batch 740/938.0, Batch Loss: 98.579424\n",
      "batch 741/938.0, Batch Loss: 105.415224\n",
      "batch 742/938.0, Batch Loss: 104.335887\n",
      "batch 743/938.0, Batch Loss: 87.786057\n",
      "batch 744/938.0, Batch Loss: 91.024067\n",
      "batch 745/938.0, Batch Loss: 94.621856\n",
      "batch 746/938.0, Batch Loss: 112.251023\n",
      "batch 747/938.0, Batch Loss: 96.060972\n",
      "batch 748/938.0, Batch Loss: 104.695666\n",
      "batch 749/938.0, Batch Loss: 99.658761\n",
      "batch 750/938.0, Batch Loss: 94.981635\n",
      "batch 751/938.0, Batch Loss: 95.341414\n",
      "batch 752/938.0, Batch Loss: 103.256550\n",
      "batch 753/938.0, Batch Loss: 101.457656\n",
      "batch 754/938.0, Batch Loss: 114.769476\n",
      "batch 755/938.0, Batch Loss: 94.621856\n",
      "batch 756/938.0, Batch Loss: 99.298982\n",
      "batch 757/938.0, Batch Loss: 96.060972\n",
      "batch 758/938.0, Batch Loss: 95.701193\n",
      "batch 759/938.0, Batch Loss: 107.573897\n",
      "batch 760/938.0, Batch Loss: 107.933676\n",
      "batch 761/938.0, Batch Loss: 108.293455\n",
      "batch 762/938.0, Batch Loss: 102.536992\n",
      "batch 763/938.0, Batch Loss: 99.658761\n",
      "batch 764/938.0, Batch Loss: 93.182740\n",
      "batch 765/938.0, Batch Loss: 106.854339\n",
      "batch 766/938.0, Batch Loss: 110.452129\n",
      "batch 767/938.0, Batch Loss: 102.177214\n",
      "batch 768/938.0, Batch Loss: 87.786057\n",
      "batch 769/938.0, Batch Loss: 103.616329\n",
      "batch 770/938.0, Batch Loss: 91.383846\n",
      "batch 771/938.0, Batch Loss: 94.621856\n",
      "batch 772/938.0, Batch Loss: 114.769476\n",
      "batch 773/938.0, Batch Loss: 111.531465\n",
      "batch 774/938.0, Batch Loss: 115.489034\n",
      "batch 775/938.0, Batch Loss: 102.177214\n",
      "batch 776/938.0, Batch Loss: 117.647707\n",
      "batch 777/938.0, Batch Loss: 107.214118\n",
      "batch 778/938.0, Batch Loss: 107.933676\n",
      "batch 779/938.0, Batch Loss: 103.616329\n",
      "batch 780/938.0, Batch Loss: 92.463183\n",
      "batch 781/938.0, Batch Loss: 81.310036\n",
      "batch 782/938.0, Batch Loss: 98.579424\n",
      "batch 783/938.0, Batch Loss: 100.018540\n",
      "batch 784/938.0, Batch Loss: 88.865393\n",
      "batch 785/938.0, Batch Loss: 109.372792\n",
      "batch 786/938.0, Batch Loss: 95.701193\n",
      "batch 787/938.0, Batch Loss: 106.854339\n",
      "batch 788/938.0, Batch Loss: 111.531465\n",
      "batch 789/938.0, Batch Loss: 92.822962\n",
      "batch 790/938.0, Batch Loss: 108.653234\n",
      "batch 791/938.0, Batch Loss: 84.907825\n",
      "batch 792/938.0, Batch Loss: 118.007486\n",
      "batch 793/938.0, Batch Loss: 97.500088\n",
      "batch 794/938.0, Batch Loss: 109.013013\n",
      "batch 795/938.0, Batch Loss: 101.457656\n",
      "batch 796/938.0, Batch Loss: 105.055445\n",
      "batch 797/938.0, Batch Loss: 104.695666\n",
      "batch 798/938.0, Batch Loss: 120.885717\n",
      "batch 799/938.0, Batch Loss: 109.013013\n",
      "batch 800/938.0, Batch Loss: 102.896771\n",
      "batch 801/938.0, Batch Loss: 94.621856\n",
      "batch 802/938.0, Batch Loss: 104.695666\n",
      "batch 803/938.0, Batch Loss: 98.939203\n",
      "batch 804/938.0, Batch Loss: 88.865393\n",
      "batch 805/938.0, Batch Loss: 101.097877\n",
      "batch 806/938.0, Batch Loss: 128.800854\n",
      "batch 807/938.0, Batch Loss: 105.775003\n",
      "batch 808/938.0, Batch Loss: 117.647707\n",
      "batch 809/938.0, Batch Loss: 106.854339\n",
      "batch 810/938.0, Batch Loss: 102.177214\n",
      "batch 811/938.0, Batch Loss: 98.219645\n",
      "batch 812/938.0, Batch Loss: 94.262077\n",
      "batch 813/938.0, Batch Loss: 109.013013\n",
      "batch 814/938.0, Batch Loss: 107.933676\n",
      "batch 815/938.0, Batch Loss: 94.621856\n",
      "batch 816/938.0, Batch Loss: 101.817435\n",
      "batch 817/938.0, Batch Loss: 88.145836\n",
      "batch 818/938.0, Batch Loss: 112.610802\n",
      "batch 819/938.0, Batch Loss: 101.817435\n",
      "batch 820/938.0, Batch Loss: 92.103404\n",
      "batch 821/938.0, Batch Loss: 100.738098\n",
      "batch 822/938.0, Batch Loss: 89.944730\n",
      "batch 823/938.0, Batch Loss: 110.092350\n",
      "batch 824/938.0, Batch Loss: 99.298982\n",
      "batch 825/938.0, Batch Loss: 94.981635\n",
      "batch 826/938.0, Batch Loss: 106.494561\n",
      "batch 827/938.0, Batch Loss: 109.013013\n",
      "batch 828/938.0, Batch Loss: 96.060972\n",
      "batch 829/938.0, Batch Loss: 105.055445\n",
      "batch 830/938.0, Batch Loss: 98.219645\n",
      "batch 831/938.0, Batch Loss: 94.981635\n",
      "batch 832/938.0, Batch Loss: 104.335887\n",
      "batch 833/938.0, Batch Loss: 97.859866\n",
      "batch 834/938.0, Batch Loss: 102.536992\n",
      "batch 835/938.0, Batch Loss: 112.610802\n",
      "batch 836/938.0, Batch Loss: 108.293455\n",
      "batch 837/938.0, Batch Loss: 101.457656\n",
      "batch 838/938.0, Batch Loss: 112.610802\n",
      "batch 839/938.0, Batch Loss: 89.584951\n",
      "batch 840/938.0, Batch Loss: 97.859866\n",
      "batch 841/938.0, Batch Loss: 107.214118\n",
      "batch 842/938.0, Batch Loss: 94.621856\n",
      "batch 843/938.0, Batch Loss: 106.854339\n",
      "batch 844/938.0, Batch Loss: 111.531465\n",
      "batch 845/938.0, Batch Loss: 90.664288\n",
      "batch 846/938.0, Batch Loss: 94.621856\n",
      "batch 847/938.0, Batch Loss: 110.811908\n",
      "batch 848/938.0, Batch Loss: 100.738098\n",
      "batch 849/938.0, Batch Loss: 91.383846\n",
      "batch 850/938.0, Batch Loss: 109.013013\n",
      "batch 851/938.0, Batch Loss: 103.976108\n",
      "batch 852/938.0, Batch Loss: 107.214118\n",
      "batch 853/938.0, Batch Loss: 86.706720\n",
      "batch 854/938.0, Batch Loss: 94.981635\n",
      "batch 855/938.0, Batch Loss: 98.579424\n",
      "batch 856/938.0, Batch Loss: 96.420751\n",
      "batch 857/938.0, Batch Loss: 108.293455\n",
      "batch 858/938.0, Batch Loss: 98.579424\n",
      "batch 859/938.0, Batch Loss: 89.944730\n",
      "batch 860/938.0, Batch Loss: 95.701193\n",
      "batch 861/938.0, Batch Loss: 105.415224\n",
      "batch 862/938.0, Batch Loss: 86.346941\n",
      "batch 863/938.0, Batch Loss: 122.324833\n",
      "batch 864/938.0, Batch Loss: 98.579424\n",
      "batch 865/938.0, Batch Loss: 101.097877\n",
      "batch 866/938.0, Batch Loss: 93.182740\n",
      "batch 867/938.0, Batch Loss: 121.245496\n",
      "batch 868/938.0, Batch Loss: 91.743625\n",
      "batch 869/938.0, Batch Loss: 105.055445\n",
      "batch 870/938.0, Batch Loss: 98.219645\n",
      "batch 871/938.0, Batch Loss: 96.420751\n",
      "batch 872/938.0, Batch Loss: 92.463183\n",
      "batch 873/938.0, Batch Loss: 120.525938\n",
      "batch 874/938.0, Batch Loss: 104.335887\n",
      "batch 875/938.0, Batch Loss: 102.536992\n",
      "batch 876/938.0, Batch Loss: 91.743625\n",
      "batch 877/938.0, Batch Loss: 101.097877\n",
      "batch 878/938.0, Batch Loss: 92.103404\n",
      "batch 879/938.0, Batch Loss: 100.378319\n",
      "batch 880/938.0, Batch Loss: 101.457656\n",
      "batch 881/938.0, Batch Loss: 103.616329\n",
      "batch 882/938.0, Batch Loss: 89.225172\n",
      "batch 883/938.0, Batch Loss: 76.273131\n",
      "batch 884/938.0, Batch Loss: 94.262077\n",
      "batch 885/938.0, Batch Loss: 97.140309\n",
      "batch 886/938.0, Batch Loss: 93.182740\n",
      "batch 887/938.0, Batch Loss: 101.457656\n",
      "batch 888/938.0, Batch Loss: 105.775003\n",
      "batch 889/938.0, Batch Loss: 98.219645\n",
      "batch 890/938.0, Batch Loss: 97.859866\n",
      "batch 891/938.0, Batch Loss: 100.738098\n",
      "batch 892/938.0, Batch Loss: 98.219645\n",
      "batch 893/938.0, Batch Loss: 103.976108\n",
      "batch 894/938.0, Batch Loss: 119.806381\n",
      "batch 895/938.0, Batch Loss: 93.182740\n",
      "batch 896/938.0, Batch Loss: 111.891244\n",
      "batch 897/938.0, Batch Loss: 112.251023\n",
      "batch 898/938.0, Batch Loss: 112.970581\n",
      "batch 899/938.0, Batch Loss: 96.420751\n",
      "batch 900/938.0, Batch Loss: 119.446602\n",
      "batch 901/938.0, Batch Loss: 103.256550\n",
      "batch 902/938.0, Batch Loss: 109.372792\n",
      "batch 903/938.0, Batch Loss: 110.811908\n",
      "batch 904/938.0, Batch Loss: 106.854339\n",
      "batch 905/938.0, Batch Loss: 112.251023\n",
      "batch 906/938.0, Batch Loss: 100.738098\n",
      "batch 907/938.0, Batch Loss: 98.579424\n",
      "batch 908/938.0, Batch Loss: 109.732571\n",
      "batch 909/938.0, Batch Loss: 97.859866\n",
      "batch 910/938.0, Batch Loss: 102.536992\n",
      "batch 911/938.0, Batch Loss: 99.658761\n",
      "batch 912/938.0, Batch Loss: 114.769476\n",
      "batch 913/938.0, Batch Loss: 97.500088\n",
      "batch 914/938.0, Batch Loss: 122.324833\n",
      "batch 915/938.0, Batch Loss: 125.922622\n",
      "batch 916/938.0, Batch Loss: 103.976108\n",
      "batch 917/938.0, Batch Loss: 111.891244\n",
      "batch 918/938.0, Batch Loss: 104.695666\n",
      "batch 919/938.0, Batch Loss: 101.097877\n",
      "batch 920/938.0, Batch Loss: 112.970581\n",
      "batch 921/938.0, Batch Loss: 96.780530\n",
      "batch 922/938.0, Batch Loss: 99.658761\n",
      "batch 923/938.0, Batch Loss: 100.018540\n",
      "batch 924/938.0, Batch Loss: 90.304509\n",
      "batch 925/938.0, Batch Loss: 102.177214\n",
      "batch 926/938.0, Batch Loss: 93.902298\n",
      "batch 927/938.0, Batch Loss: 106.494561\n",
      "batch 928/938.0, Batch Loss: 103.976108\n",
      "batch 929/938.0, Batch Loss: 95.701193\n",
      "batch 930/938.0, Batch Loss: 97.500088\n",
      "batch 931/938.0, Batch Loss: 85.627383\n",
      "batch 932/938.0, Batch Loss: 101.097877\n",
      "batch 933/938.0, Batch Loss: 91.743625\n",
      "batch 934/938.0, Batch Loss: 106.854339\n",
      "batch 935/938.0, Batch Loss: 112.251023\n",
      "batch 936/938.0, Batch Loss: 100.018540\n",
      "batch 937/938.0, Batch Loss: 82.749152\n",
      "Epoch 1/1, Average Loss: 127.169801, Validation Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_dim = 784        # Number of input features\n",
    "    hidden1 = 128          # Number of neurons in first hidden layer\n",
    "    hidden2 = 128          # Number of neurons in second hidden layer\n",
    "    num_classes = 10       # Number of output classes\n",
    "\n",
    "    layer_sizes = [input_dim, hidden1, hidden2, num_classes]\n",
    "    activation_functions = ['relu', 'relu', 'linear']  # outputs logits\n",
    "\n",
    "    model = MLP(\n",
    "        layer_sizes=layer_sizes,\n",
    "        activation_functions=activation_functions\n",
    "    )\n",
    "\n",
    "    training_data = X_train/255.0\n",
    "    training_targets = y_train\n",
    "    validation_data = X_test/255.0\n",
    "    validation_targets = y_test\n",
    "\n",
    "    epochs = 1\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    model.train(\n",
    "        training_data=training_data,\n",
    "        training_targets=training_targets,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        validation_data=validation_data,\n",
    "        validation_targets=validation_targets,\n",
    "        batch_size=64\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.compute_accuracy(validation_data, validation_targets)\n",
    "print(f\"{accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
