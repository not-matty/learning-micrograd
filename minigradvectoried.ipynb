{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.304229781651375\n",
      "Epoch 2, Loss: 2.304246532141078\n",
      "Epoch 3, Loss: 2.3042389861327033\n",
      "Epoch 4, Loss: 2.3042450738534472\n",
      "Epoch 5, Loss: 2.3042416304869584\n",
      "Epoch 6, Loss: 2.3042460072738375\n",
      "Epoch 7, Loss: 2.304235586380251\n",
      "Epoch 8, Loss: 2.30423101829231\n",
      "Epoch 9, Loss: 2.3042455857886393\n",
      "Epoch 10, Loss: 2.3042517581310467\n",
      "Epoch 11, Loss: 2.3042415998487633\n",
      "Epoch 12, Loss: 2.304237221431178\n",
      "Epoch 13, Loss: 2.3042469299317285\n",
      "Epoch 14, Loss: 2.304239810198513\n",
      "Epoch 15, Loss: 2.30424606657805\n",
      "Epoch 16, Loss: 2.3042382420315817\n",
      "Epoch 17, Loss: 2.304248043134171\n",
      "Epoch 18, Loss: 2.3042407357412698\n",
      "Epoch 19, Loss: 2.304237094757015\n",
      "Epoch 20, Loss: 2.304248916864858\n",
      "Epoch 21, Loss: 2.3042524454588604\n",
      "Epoch 22, Loss: 2.304240381057896\n",
      "Epoch 23, Loss: 2.304242399751738\n",
      "Epoch 24, Loss: 2.3042424035672777\n",
      "Epoch 25, Loss: 2.3042466082818174\n",
      "Epoch 26, Loss: 2.3042336726432784\n",
      "Epoch 27, Loss: 2.30424259993602\n",
      "Epoch 28, Loss: 2.3042454270067645\n",
      "Epoch 29, Loss: 2.304245961183281\n",
      "Epoch 30, Loss: 2.3042415406344676\n"
     ]
    }
   ],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.weight = np.random.randn(out_features, in_features) * np.sqrt(2.0 / in_features)\n",
    "        self.bias = np.zeros(out_features) if bias else None\n",
    "        self.grad_weight = np.zeros_like(self.weight)\n",
    "        self.grad_bias = np.zeros_like(self.bias) if bias else None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.matmul(X, self.weight.T) + self.bias\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.grad_weight = np.matmul(grad_output.T, self.input)\n",
    "        if self.bias is not None:\n",
    "            self.grad_bias = np.sum(grad_output, axis=0)\n",
    "        return np.matmul(grad_output, self.weight)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad_weight.fill(0)\n",
    "        if self.bias is not None:\n",
    "            self.grad_bias.fill(0)\n",
    "\n",
    "    def parameters(self):\n",
    "        params = [{'value': self.weight, 'grad': self.grad_weight}]\n",
    "        if self.bias is not None:\n",
    "            params.append({'value': self.bias, 'grad': self.grad_bias})\n",
    "        return params\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input > 0)\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        self.output = exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad_output = layer.backward(grad_output)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'zero_grad'):\n",
    "                layer.zero_grad()\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'parameters'):\n",
    "                params.extend(layer.parameters())\n",
    "        return params\n",
    "\n",
    "def CrossEntropyLoss(y_true, y_pred, model, lambda_reg=0.00001):\n",
    "    y_true = y_true.flatten()\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)  # Prevent log(0)\n",
    "    loss = -np.log(y_pred[np.arange(len(y_true)), y_true])\n",
    "    # L2 Regularization Term: Sum of squared weights\n",
    "    l2_reg = 0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += np.sum(param['value'] ** 2)\n",
    "    # Include the regularization term in the loss\n",
    "    return np.mean(loss) + (lambda_reg / 2) * l2_reg\n",
    "\n",
    "def CrossEntropyLossBackward(y_true, y_pred):\n",
    "    y_true = y_true.flatten()\n",
    "    grad = y_pred.copy()\n",
    "    grad[np.arange(len(y_true)), y_true] -= 1\n",
    "    return grad / len(y_true)\n",
    "\n",
    "class StochasticGradientDescentWithMomentum:\n",
    "    def __init__(self, parameters, lr, momentum=0.9):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        # Initialize velocity for each parameter gradient to zero\n",
    "        self.velocities = [{'grad': np.zeros_like(param['grad'])} for param in parameters]\n",
    "\n",
    "    def step(self):\n",
    "        # Update parameters with momentum\n",
    "        for param, velocity in zip(self.parameters, self.velocities):\n",
    "            # Compute the velocity update with the momentum term\n",
    "            velocity['grad'] = self.momentum * velocity['grad'] + param['grad']\n",
    "            # Update the parameter using the velocity scaled by learning rate\n",
    "            param['value'] -= self.lr * velocity['grad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('/Users/matthew/Downloads/archive/mnist_train.csv')\n",
    "test_data = pd.read_csv('/Users/matthew/Downloads/archive/mnist_test.csv')\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_label shape: (60000, 1)\n",
      "x_train shape: (60000, 784)\n",
      "x_test shape: (10000, 1)\n",
      "x_test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data.iloc[:, [0]]\n",
    "X_train = train_data.iloc[:, 1:]\n",
    "y_test = test_data.iloc[:, [0]]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"x_label shape: {y_train.shape}\")\n",
    "print(f\"x_train shape: {X_train.shape}\")\n",
    "print(f\"x_test shape: {y_test.shape}\")\n",
    "print(f\"x_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy()\n",
    "X_train = X_train.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "X_test = X_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_train[1]))\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(Linear(784, 128))\n",
    "model.add(ReLU())\n",
    "model.add(Linear(128, 32))\n",
    "model.add(ReLU())\n",
    "model.add(Linear(32, 10))\n",
    "model.add(Softmax())\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = StochasticGradientDescentWithMomentum(model.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "# Training settings\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "num_batches = X_train.shape[0] // batch_size\n",
    "\n",
    "# Normalizing the input data\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.zero_grad()  # Reset gradients at the start of each epoch\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in range(num_batches):\n",
    "        # Randomly select a batch of indices\n",
    "        batch_indices = np.random.choice(X_train.shape[0], batch_size, replace=False)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        predictions = model.forward(X_batch)\n",
    "\n",
    "        loss = CrossEntropyLoss(y_batch, predictions, model, lambda_reg=0.00001)\n",
    "        epoch_loss += loss\n",
    "\n",
    "        grad_loss = CrossEntropyLossBackward(y_batch, predictions)\n",
    "        model.backward(grad_loss)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # Averaging loss over the number of batches\n",
    "    epoch_loss /= num_batches\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.51%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_accuracy(model, x_test, y_test):\n",
    "    predictions = model.forward(x_test)\n",
    "    \n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    true_labels = y_test.flatten()\n",
    "    \n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_accuracy(model, X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
