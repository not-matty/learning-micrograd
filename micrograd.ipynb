{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    Converts integer labels to one-hot encoded vectors.\n",
    "\n",
    "    Args:\n",
    "        labels (numpy.ndarray): Array of integer labels.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: One-hot encoded labels.\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels].reshape(-1, num_classes)\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nin, activation=None):\n",
    "        \"\"\"\n",
    "        Initializes a neuron with random weights and bias.\n",
    "\n",
    "        Args:\n",
    "            nin (int): Number of inputs to the neuron.\n",
    "            activation (str): Activation function to use ('sigmoid', 'relu', 'tanh', or 'linear').\n",
    "        \"\"\"\n",
    "        if activation in ['relu']:\n",
    "            std = np.sqrt(2. / nin)\n",
    "        elif activation in ['tanh', 'sigmoid']:\n",
    "            std = np.sqrt(1. / nin)\n",
    "        else:\n",
    "            std = np.sqrt(1. / nin)\n",
    "        self.weights = np.random.normal(0, std, nin)\n",
    "        self.bias = 0.0  # Initialize bias to zero\n",
    "        self.activation = activation\n",
    "\n",
    "        self.input_values = None\n",
    "        self.weighted_sum = 0.0\n",
    "        self.output_value = 0.0\n",
    "\n",
    "        self.weight_gradients = np.zeros(nin)\n",
    "        self.bias_gradient = 0.0\n",
    "        self.input_gradients = np.zeros(nin)\n",
    "\n",
    "    def activate(self, z):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0.0, z)\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(z)\n",
    "        else:\n",
    "            return z  # linear activation\n",
    "\n",
    "    def activation_derivative(self, activated_output):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return activated_output * (1 - activated_output)\n",
    "        elif self.activation == 'relu':\n",
    "            return 1.0 if activated_output > 0 else 0.0\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - activated_output ** 2\n",
    "        else:\n",
    "            return 1.0  # derivative of linear activation\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the neuron.\n",
    "\n",
    "        Args:\n",
    "            input_values (numpy.ndarray): Inputs to the neuron.\n",
    "\n",
    "        Returns:\n",
    "            float: Output after applying activation function.\n",
    "        \"\"\"\n",
    "        self.input_values = input_values\n",
    "        self.weighted_sum = np.dot(self.weights, input_values) + self.bias\n",
    "        self.output_value = self.activate(self.weighted_sum)\n",
    "        return self.output_value\n",
    "\n",
    "    def backward(self, gradient_from_above):\n",
    "        \"\"\"\n",
    "        Performs the backward pass for the neuron.\n",
    "\n",
    "        Args:\n",
    "            gradient_from_above (float): Gradient from the next layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to pass to the previous layer.\n",
    "        \"\"\"\n",
    "        activation_derivative = self.activation_derivative(self.output_value)\n",
    "        gradient_weighted_sum = gradient_from_above * activation_derivative\n",
    "\n",
    "        # Compute gradients for weights and bias\n",
    "        self.weight_gradients = gradient_weighted_sum * self.input_values\n",
    "        self.bias_gradient = gradient_weighted_sum\n",
    "\n",
    "        # Compute gradients for inputs to propagate to previous layer\n",
    "        self.input_gradients = gradient_weighted_sum * self.weights\n",
    "        return self.input_gradients\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, units, nin, activation=None):\n",
    "        \"\"\"\n",
    "        Initializes a layer with a specified number of neurons.\n",
    "\n",
    "        Args:\n",
    "            units (int): Number of neurons in the layer.\n",
    "            nin (int): Number of inputs each neuron receives.\n",
    "            activation (str): Activation function for the neurons ('sigmoid', 'relu', 'tanh', 'linear', 'softmax').\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.neurons = [Neuron(nin, activation=None if activation == 'softmax' else activation) for _ in range(units)]\n",
    "        self.output_values = None\n",
    "\n",
    "    def forward(self, input_values):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for the layer.\n",
    "\n",
    "        Args:\n",
    "            input_values (numpy.ndarray): Inputs to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Outputs from the layer.\n",
    "        \"\"\"\n",
    "        # Collect outputs from all neurons in the layer\n",
    "        neuron_outputs = np.array([neuron.forward(input_values) for neuron in self.neurons])\n",
    "\n",
    "        if self.activation == 'softmax':\n",
    "            # Softmax will be handled separately in the MLP class\n",
    "            self.output_values = neuron_outputs\n",
    "        else:\n",
    "            self.output_values = neuron_outputs\n",
    "\n",
    "        return self.output_values\n",
    "\n",
    "    def backward(self, gradients_from_above, clip_value=1.0):\n",
    "        \"\"\"\n",
    "        Performs the backward pass for the layer.\n",
    "\n",
    "        Args:\n",
    "            gradients_from_above (numpy.ndarray): Gradients from the next layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradients to pass to the previous layer.\n",
    "        \"\"\"\n",
    "        gradients_to_previous_layer = np.zeros_like(self.neurons[0].input_gradients)\n",
    "        \n",
    "        for neuron, grad in zip(self.neurons, gradients_from_above):\n",
    "            clipped_grad = np.clip(grad, -clip_value, clip_value)\n",
    "            neuron_input_gradients = neuron.backward(clipped_grad)\n",
    "            gradients_to_previous_layer += neuron_input_gradients\n",
    "    \n",
    "        return gradients_to_previous_layer\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Updates the parameters (weights and biases) of all neurons in the layer.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for the update step.\n",
    "        \"\"\"\n",
    "        for neuron in self.neurons:\n",
    "            # Update weights and biases using gradients\n",
    "            neuron.weights -= learning_rate * neuron.weight_gradients\n",
    "            neuron.bias -= learning_rate * neuron.bias_gradient\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activation_functions):\n",
    "        \"\"\"\n",
    "        Initializes the Multi-Layer Perceptron.\n",
    "        \"\"\"\n",
    "        assert len(layer_sizes) - 1 == len(activation_functions), \"Mismatch between layer sizes and activation functions.\"\n",
    "        self.loss_history = []\n",
    "        self.layers = []\n",
    "        for i in range(1, len(layer_sizes)):\n",
    "            self.layers.append(\n",
    "                Layer(\n",
    "                    units=layer_sizes[i],\n",
    "                    nin=layer_sizes[i - 1],\n",
    "                    activation=activation_functions[i - 1]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        for layer in self.layers:\n",
    "            input_data = layer.forward(input_data)\n",
    "        return input_data  # Returns logits\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        logits = np.nan_to_num(logits, nan=0.0, posinf=500, neginf=-500)\n",
    "        logits = np.clip(logits, -500, 500)\n",
    "        max_logit = np.max(logits)\n",
    "        exps = np.exp(logits - max_logit)\n",
    "        sum_exps = np.sum(exps)\n",
    "        probabilities = exps / sum_exps\n",
    "        return probabilities\n",
    "\n",
    "    def compute_loss(self, predicted_logits, target_outputs):\n",
    "        probabilities = self.softmax(predicted_logits)  # shape: (num_classes,)\n",
    "        epsilon = 1e-15\n",
    "        probabilities = np.clip(probabilities, epsilon, 1 - epsilon)\n",
    "        loss = -np.sum(target_outputs * np.log(probabilities))\n",
    "        return loss\n",
    "\n",
    "    def compute_loss_derivative(self, predicted_logits, target_outputs):\n",
    "        probabilities = self.softmax(predicted_logits)\n",
    "        gradients = probabilities - target_outputs\n",
    "        return gradients\n",
    "\n",
    "    def backward(self, loss_gradients):\n",
    "        gradients = loss_gradients\n",
    "        for layer in reversed(self.layers):\n",
    "            gradients = layer.backward(np.clip(gradients, -1.0, 1.0))\n",
    "\n",
    "    def update_parameters(self, learning_rate):\n",
    "        for layer in self.layers:\n",
    "            layer.update_parameters(learning_rate)\n",
    "\n",
    "    def train(self, training_data, training_targets, epochs, learning_rate, validation_data=None, validation_targets=None, batch_size=64):\n",
    "        # One-hot encode the training and validation targets\n",
    "        training_targets_one_hot = one_hot_encode(training_targets, num_classes=10)\n",
    "        if validation_targets is not None:\n",
    "            validation_targets_one_hot = one_hot_encode(validation_targets, num_classes=10)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Shuffle data\n",
    "            indices = np.arange(len(training_data))\n",
    "            np.random.shuffle(indices)\n",
    "            training_data = training_data[indices]\n",
    "            training_targets_one_hot = training_targets_one_hot[indices]\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            num_batches = int(np.ceil(len(training_data) / batch_size))\n",
    "            \n",
    "            for batch_idx in range(num_batches):\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                batch_inputs = training_data[start:end]\n",
    "                batch_targets = training_targets_one_hot[start:end]\n",
    "                \n",
    "                batch_loss = 0.0\n",
    "                batch_gradients = np.zeros_like(self.forward(batch_inputs[0]))\n",
    "                \n",
    "                for inputs, target in zip(batch_inputs, batch_targets):\n",
    "                    logits = self.forward(inputs)\n",
    "                    loss = self.compute_loss(logits, target)\n",
    "                    batch_loss += loss\n",
    "                    loss_grad = self.compute_loss_derivative(logits, target)\n",
    "                    batch_gradients += loss_grad\n",
    "                \n",
    "                # Average gradients and loss\n",
    "                batch_gradients /= len(batch_inputs)\n",
    "                batch_loss /= len(batch_inputs)\n",
    "                total_loss += batch_loss\n",
    "                print(f\"Batch {batch_idx+1}/{num_batches}, Batch Loss: {batch_loss:.6f}\")\n",
    "\n",
    "                # Backward pass and update\n",
    "                self.backward(batch_gradients)\n",
    "                self.update_parameters(learning_rate)\n",
    "            \n",
    "            average_loss = total_loss / num_batches\n",
    "            self.loss_history.append(average_loss)\n",
    "            \n",
    "            # Print loss and accuracy every epoch\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                if validation_data is not None and validation_targets is not None:\n",
    "                    accuracy = self.compute_accuracy(validation_data, validation_targets)\n",
    "                    print(f\"Epoch {epoch}/{epochs}, Average Loss: {average_loss:.6f}, Validation Accuracy: {accuracy:.2%}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch}/{epochs}, Average Loss: {average_loss:.6f}\")\n",
    "\n",
    "    def compute_accuracy(self, data, targets):\n",
    "        correct = 0\n",
    "        for inputs, target in zip(data, targets):\n",
    "            logits = self.forward(inputs)\n",
    "            probabilities = self.softmax(logits)\n",
    "            predicted_class = np.argmax(probabilities)\n",
    "            true_class = target  # Assuming target is an integer label\n",
    "            if predicted_class == true_class:\n",
    "                correct += 1\n",
    "        accuracy = correct / len(data)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('/Users/matthew/Downloads/archive/mnist_train.csv')\n",
    "test_data = pd.read_csv('/Users/matthew/Downloads/archive/mnist_test.csv')\n",
    "\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_label shape: (60000, 1)\n",
      "x_train shape: (60000, 784)\n",
      "x_test shape: (10000, 1)\n",
      "x_test shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data.iloc[:, [0]]\n",
    "X_train = train_data.iloc[:, 1:]\n",
    "y_test = test_data.iloc[:, [0]]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "\n",
    "print(f\"x_label shape: {y_train.shape}\")\n",
    "print(f\"x_train shape: {X_train.shape}\")\n",
    "print(f\"x_test shape: {y_test.shape}\")\n",
    "print(f\"x_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.values\n",
    "X_train = X_train.values\n",
    "y_test = y_test.values\n",
    "X_test = X_test.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "7\n",
      "[[7]\n",
      " [2]\n",
      " [1]\n",
      " ...\n",
      " [4]\n",
      " [5]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_train[0]))\n",
    "print(X_train)\n",
    "print(np.max(y_test[0]))\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/600, Batch Loss: 2.417559\n",
      "Batch 2/600, Batch Loss: 2.364747\n",
      "Batch 3/600, Batch Loss: 2.371964\n",
      "Batch 4/600, Batch Loss: 2.382036\n",
      "Batch 5/600, Batch Loss: 2.378985\n",
      "Batch 6/600, Batch Loss: 2.420457\n",
      "Batch 7/600, Batch Loss: 2.394989\n",
      "Batch 8/600, Batch Loss: 2.301881\n",
      "Batch 9/600, Batch Loss: 2.371905\n",
      "Batch 10/600, Batch Loss: 2.358880\n",
      "Batch 11/600, Batch Loss: 2.378020\n",
      "Batch 12/600, Batch Loss: 2.344229\n",
      "Batch 13/600, Batch Loss: 2.350678\n",
      "Batch 14/600, Batch Loss: 2.358543\n",
      "Batch 15/600, Batch Loss: 2.393341\n",
      "Batch 16/600, Batch Loss: 2.335837\n",
      "Batch 17/600, Batch Loss: 2.343388\n",
      "Batch 18/600, Batch Loss: 2.334191\n",
      "Batch 19/600, Batch Loss: 2.390951\n",
      "Batch 20/600, Batch Loss: 2.407946\n",
      "Batch 21/600, Batch Loss: 2.390086\n",
      "Batch 22/600, Batch Loss: 2.330932\n",
      "Batch 23/600, Batch Loss: 2.301200\n",
      "Batch 24/600, Batch Loss: 2.341489\n",
      "Batch 25/600, Batch Loss: 2.356918\n",
      "Batch 26/600, Batch Loss: 2.341426\n",
      "Batch 27/600, Batch Loss: 2.370196\n",
      "Batch 28/600, Batch Loss: 2.320971\n",
      "Batch 29/600, Batch Loss: 2.374167\n",
      "Batch 30/600, Batch Loss: 2.348764\n",
      "Batch 31/600, Batch Loss: 2.316940\n",
      "Batch 32/600, Batch Loss: 2.344469\n",
      "Batch 33/600, Batch Loss: 2.338568\n",
      "Batch 34/600, Batch Loss: 2.318636\n",
      "Batch 35/600, Batch Loss: 2.332278\n",
      "Batch 36/600, Batch Loss: 2.351875\n",
      "Batch 37/600, Batch Loss: 2.353398\n",
      "Batch 38/600, Batch Loss: 2.317453\n",
      "Batch 39/600, Batch Loss: 2.322366\n",
      "Batch 40/600, Batch Loss: 2.349769\n",
      "Batch 41/600, Batch Loss: 2.369343\n",
      "Batch 42/600, Batch Loss: 2.389707\n",
      "Batch 43/600, Batch Loss: 2.319481\n",
      "Batch 44/600, Batch Loss: 2.344459\n",
      "Batch 45/600, Batch Loss: 2.326797\n",
      "Batch 46/600, Batch Loss: 2.324862\n",
      "Batch 47/600, Batch Loss: 2.313395\n",
      "Batch 48/600, Batch Loss: 2.351660\n",
      "Batch 49/600, Batch Loss: 2.339358\n",
      "Batch 50/600, Batch Loss: 2.340237\n",
      "Batch 51/600, Batch Loss: 2.313059\n",
      "Batch 52/600, Batch Loss: 2.344987\n",
      "Batch 53/600, Batch Loss: 2.330118\n",
      "Batch 54/600, Batch Loss: 2.335156\n",
      "Batch 55/600, Batch Loss: 2.342323\n",
      "Batch 56/600, Batch Loss: 2.294530\n",
      "Batch 57/600, Batch Loss: 2.363312\n",
      "Batch 58/600, Batch Loss: 2.343609\n",
      "Batch 59/600, Batch Loss: 2.389082\n",
      "Batch 60/600, Batch Loss: 2.325565\n",
      "Batch 61/600, Batch Loss: 2.367643\n",
      "Batch 62/600, Batch Loss: 2.336267\n",
      "Batch 63/600, Batch Loss: 2.345149\n",
      "Batch 64/600, Batch Loss: 2.266485\n",
      "Batch 65/600, Batch Loss: 2.358595\n",
      "Batch 66/600, Batch Loss: 2.338916\n",
      "Batch 67/600, Batch Loss: 2.350849\n",
      "Batch 68/600, Batch Loss: 2.356969\n",
      "Batch 69/600, Batch Loss: 2.333549\n",
      "Batch 70/600, Batch Loss: 2.346683\n",
      "Batch 71/600, Batch Loss: 2.329604\n",
      "Batch 72/600, Batch Loss: 2.368197\n",
      "Batch 73/600, Batch Loss: 2.346491\n",
      "Batch 74/600, Batch Loss: 2.351771\n",
      "Batch 75/600, Batch Loss: 2.363922\n",
      "Batch 76/600, Batch Loss: 2.374227\n",
      "Batch 77/600, Batch Loss: 2.369628\n",
      "Batch 78/600, Batch Loss: 2.293917\n",
      "Batch 79/600, Batch Loss: 2.334217\n",
      "Batch 80/600, Batch Loss: 2.320858\n",
      "Batch 81/600, Batch Loss: 2.315832\n",
      "Batch 82/600, Batch Loss: 2.311251\n",
      "Batch 83/600, Batch Loss: 2.353441\n",
      "Batch 84/600, Batch Loss: 2.310586\n",
      "Batch 85/600, Batch Loss: 2.351465\n",
      "Batch 86/600, Batch Loss: 2.376675\n",
      "Batch 87/600, Batch Loss: 2.312259\n",
      "Batch 88/600, Batch Loss: 2.302121\n",
      "Batch 89/600, Batch Loss: 2.364750\n",
      "Batch 90/600, Batch Loss: 2.338577\n",
      "Batch 91/600, Batch Loss: 2.312416\n",
      "Batch 92/600, Batch Loss: 2.365722\n",
      "Batch 93/600, Batch Loss: 2.318113\n",
      "Batch 94/600, Batch Loss: 2.317139\n",
      "Batch 95/600, Batch Loss: 2.333513\n",
      "Batch 96/600, Batch Loss: 2.303734\n",
      "Batch 97/600, Batch Loss: 2.306225\n",
      "Batch 98/600, Batch Loss: 2.335238\n",
      "Batch 99/600, Batch Loss: 2.331493\n",
      "Batch 100/600, Batch Loss: 2.322532\n",
      "Batch 101/600, Batch Loss: 2.329035\n",
      "Batch 102/600, Batch Loss: 2.352140\n",
      "Batch 103/600, Batch Loss: 2.359755\n",
      "Batch 104/600, Batch Loss: 2.307639\n",
      "Batch 105/600, Batch Loss: 2.343306\n",
      "Batch 106/600, Batch Loss: 2.327708\n",
      "Batch 107/600, Batch Loss: 2.330087\n",
      "Batch 108/600, Batch Loss: 2.333617\n",
      "Batch 109/600, Batch Loss: 2.340852\n",
      "Batch 110/600, Batch Loss: 2.323823\n",
      "Batch 111/600, Batch Loss: 2.345066\n",
      "Batch 112/600, Batch Loss: 2.380342\n",
      "Batch 113/600, Batch Loss: 2.317761\n",
      "Batch 114/600, Batch Loss: 2.339044\n",
      "Batch 115/600, Batch Loss: 2.306545\n",
      "Batch 116/600, Batch Loss: 2.356148\n",
      "Batch 117/600, Batch Loss: 2.311984\n",
      "Batch 118/600, Batch Loss: 2.361755\n",
      "Batch 119/600, Batch Loss: 2.318491\n",
      "Batch 120/600, Batch Loss: 2.322224\n",
      "Batch 121/600, Batch Loss: 2.316380\n",
      "Batch 122/600, Batch Loss: 2.351096\n",
      "Batch 123/600, Batch Loss: 2.339508\n",
      "Batch 124/600, Batch Loss: 2.317885\n",
      "Batch 125/600, Batch Loss: 2.320739\n",
      "Batch 126/600, Batch Loss: 2.336988\n",
      "Batch 127/600, Batch Loss: 2.343893\n",
      "Batch 128/600, Batch Loss: 2.317855\n",
      "Batch 129/600, Batch Loss: 2.322061\n",
      "Batch 130/600, Batch Loss: 2.371992\n",
      "Batch 131/600, Batch Loss: 2.313033\n",
      "Batch 132/600, Batch Loss: 2.322461\n",
      "Batch 133/600, Batch Loss: 2.332788\n",
      "Batch 134/600, Batch Loss: 2.325005\n",
      "Batch 135/600, Batch Loss: 2.322542\n",
      "Batch 136/600, Batch Loss: 2.353671\n",
      "Batch 137/600, Batch Loss: 2.309578\n",
      "Batch 138/600, Batch Loss: 2.311368\n",
      "Batch 139/600, Batch Loss: 2.345538\n",
      "Batch 140/600, Batch Loss: 2.363925\n",
      "Batch 141/600, Batch Loss: 2.298191\n",
      "Batch 142/600, Batch Loss: 2.336813\n",
      "Batch 143/600, Batch Loss: 2.304222\n",
      "Batch 144/600, Batch Loss: 2.298826\n",
      "Batch 145/600, Batch Loss: 2.318577\n",
      "Batch 146/600, Batch Loss: 2.360783\n",
      "Batch 147/600, Batch Loss: 2.352204\n",
      "Batch 148/600, Batch Loss: 2.327276\n",
      "Batch 149/600, Batch Loss: 2.303479\n",
      "Batch 150/600, Batch Loss: 2.327406\n",
      "Batch 151/600, Batch Loss: 2.309628\n",
      "Batch 152/600, Batch Loss: 2.326541\n",
      "Batch 153/600, Batch Loss: 2.323041\n",
      "Batch 154/600, Batch Loss: 2.320155\n",
      "Batch 155/600, Batch Loss: 2.329905\n",
      "Batch 156/600, Batch Loss: 2.321607\n",
      "Batch 157/600, Batch Loss: 2.328754\n",
      "Batch 158/600, Batch Loss: 2.335460\n",
      "Batch 159/600, Batch Loss: 2.280160\n",
      "Batch 160/600, Batch Loss: 2.307234\n",
      "Batch 161/600, Batch Loss: 2.279988\n",
      "Batch 162/600, Batch Loss: 2.316275\n",
      "Batch 163/600, Batch Loss: 2.292749\n",
      "Batch 164/600, Batch Loss: 2.327595\n",
      "Batch 165/600, Batch Loss: 2.314786\n",
      "Batch 166/600, Batch Loss: 2.317541\n",
      "Batch 167/600, Batch Loss: 2.331126\n",
      "Batch 168/600, Batch Loss: 2.307318\n",
      "Batch 169/600, Batch Loss: 2.306106\n",
      "Batch 170/600, Batch Loss: 2.342851\n",
      "Batch 171/600, Batch Loss: 2.358868\n",
      "Batch 172/600, Batch Loss: 2.309729\n",
      "Batch 173/600, Batch Loss: 2.332431\n",
      "Batch 174/600, Batch Loss: 2.318454\n",
      "Batch 175/600, Batch Loss: 2.300059\n",
      "Batch 176/600, Batch Loss: 2.341248\n",
      "Batch 177/600, Batch Loss: 2.302213\n",
      "Batch 178/600, Batch Loss: 2.341197\n",
      "Batch 179/600, Batch Loss: 2.347954\n",
      "Batch 180/600, Batch Loss: 2.335899\n",
      "Batch 181/600, Batch Loss: 2.304809\n",
      "Batch 182/600, Batch Loss: 2.329408\n",
      "Batch 183/600, Batch Loss: 2.359014\n",
      "Batch 184/600, Batch Loss: 2.315690\n",
      "Batch 185/600, Batch Loss: 2.308762\n",
      "Batch 186/600, Batch Loss: 2.321883\n",
      "Batch 187/600, Batch Loss: 2.300660\n",
      "Batch 188/600, Batch Loss: 2.273431\n",
      "Batch 189/600, Batch Loss: 2.299287\n",
      "Batch 190/600, Batch Loss: 2.308154\n",
      "Batch 191/600, Batch Loss: 2.328701\n",
      "Batch 192/600, Batch Loss: 2.313694\n",
      "Batch 193/600, Batch Loss: 2.285149\n",
      "Batch 194/600, Batch Loss: 2.332645\n",
      "Batch 195/600, Batch Loss: 2.314488\n",
      "Batch 196/600, Batch Loss: 2.339505\n",
      "Batch 197/600, Batch Loss: 2.320574\n",
      "Batch 198/600, Batch Loss: 2.360722\n",
      "Batch 199/600, Batch Loss: 2.320997\n",
      "Batch 200/600, Batch Loss: 2.304883\n",
      "Batch 201/600, Batch Loss: 2.343889\n",
      "Batch 202/600, Batch Loss: 2.325613\n",
      "Batch 203/600, Batch Loss: 2.335385\n",
      "Batch 204/600, Batch Loss: 2.327997\n",
      "Batch 205/600, Batch Loss: 2.279948\n",
      "Batch 206/600, Batch Loss: 2.300562\n",
      "Batch 207/600, Batch Loss: 2.327203\n",
      "Batch 208/600, Batch Loss: 2.305199\n",
      "Batch 209/600, Batch Loss: 2.301513\n",
      "Batch 210/600, Batch Loss: 2.305838\n",
      "Batch 211/600, Batch Loss: 2.305770\n",
      "Batch 212/600, Batch Loss: 2.314228\n",
      "Batch 213/600, Batch Loss: 2.308039\n",
      "Batch 214/600, Batch Loss: 2.351149\n",
      "Batch 215/600, Batch Loss: 2.298862\n",
      "Batch 216/600, Batch Loss: 2.332376\n",
      "Batch 217/600, Batch Loss: 2.321819\n",
      "Batch 218/600, Batch Loss: 2.311030\n",
      "Batch 219/600, Batch Loss: 2.314965\n",
      "Batch 220/600, Batch Loss: 2.305960\n",
      "Batch 221/600, Batch Loss: 2.349456\n",
      "Batch 222/600, Batch Loss: 2.310427\n",
      "Batch 223/600, Batch Loss: 2.338275\n",
      "Batch 224/600, Batch Loss: 2.351022\n",
      "Batch 225/600, Batch Loss: 2.310744\n",
      "Batch 226/600, Batch Loss: 2.312618\n",
      "Batch 227/600, Batch Loss: 2.319866\n",
      "Batch 228/600, Batch Loss: 2.353316\n",
      "Batch 229/600, Batch Loss: 2.330426\n",
      "Batch 230/600, Batch Loss: 2.311022\n",
      "Batch 231/600, Batch Loss: 2.320877\n",
      "Batch 232/600, Batch Loss: 2.316191\n",
      "Batch 233/600, Batch Loss: 2.357027\n",
      "Batch 234/600, Batch Loss: 2.295446\n",
      "Batch 235/600, Batch Loss: 2.311414\n",
      "Batch 236/600, Batch Loss: 2.305277\n",
      "Batch 237/600, Batch Loss: 2.299613\n",
      "Batch 238/600, Batch Loss: 2.323375\n",
      "Batch 239/600, Batch Loss: 2.291456\n",
      "Batch 240/600, Batch Loss: 2.334078\n",
      "Batch 241/600, Batch Loss: 2.312159\n",
      "Batch 242/600, Batch Loss: 2.336200\n",
      "Batch 243/600, Batch Loss: 2.313329\n",
      "Batch 244/600, Batch Loss: 2.381605\n",
      "Batch 245/600, Batch Loss: 2.359805\n",
      "Batch 246/600, Batch Loss: 2.354535\n",
      "Batch 247/600, Batch Loss: 2.341131\n",
      "Batch 248/600, Batch Loss: 2.302876\n",
      "Batch 249/600, Batch Loss: 2.347435\n",
      "Batch 250/600, Batch Loss: 2.307417\n",
      "Batch 251/600, Batch Loss: 2.284648\n",
      "Batch 252/600, Batch Loss: 2.301132\n",
      "Batch 253/600, Batch Loss: 2.312829\n",
      "Batch 254/600, Batch Loss: 2.297244\n",
      "Batch 255/600, Batch Loss: 2.317995\n",
      "Batch 256/600, Batch Loss: 2.279474\n",
      "Batch 257/600, Batch Loss: 2.291066\n",
      "Batch 258/600, Batch Loss: 2.333931\n",
      "Batch 259/600, Batch Loss: 2.321726\n",
      "Batch 260/600, Batch Loss: 2.341386\n",
      "Batch 261/600, Batch Loss: 2.318321\n",
      "Batch 262/600, Batch Loss: 2.317985\n",
      "Batch 263/600, Batch Loss: 2.305447\n",
      "Batch 264/600, Batch Loss: 2.323403\n",
      "Batch 265/600, Batch Loss: 2.340457\n",
      "Batch 266/600, Batch Loss: 2.298155\n",
      "Batch 267/600, Batch Loss: 2.288330\n",
      "Batch 268/600, Batch Loss: 2.299418\n",
      "Batch 269/600, Batch Loss: 2.316494\n",
      "Batch 270/600, Batch Loss: 2.310579\n",
      "Batch 271/600, Batch Loss: 2.343323\n",
      "Batch 272/600, Batch Loss: 2.336617\n",
      "Batch 273/600, Batch Loss: 2.297419\n",
      "Batch 274/600, Batch Loss: 2.318228\n",
      "Batch 275/600, Batch Loss: 2.336069\n",
      "Batch 276/600, Batch Loss: 2.291799\n",
      "Batch 277/600, Batch Loss: 2.288445\n",
      "Batch 278/600, Batch Loss: 2.276567\n",
      "Batch 279/600, Batch Loss: 2.309391\n",
      "Batch 280/600, Batch Loss: 2.320109\n",
      "Batch 281/600, Batch Loss: 2.311155\n",
      "Batch 282/600, Batch Loss: 2.318352\n",
      "Batch 283/600, Batch Loss: 2.296686\n",
      "Batch 284/600, Batch Loss: 2.297080\n",
      "Batch 285/600, Batch Loss: 2.313755\n",
      "Batch 286/600, Batch Loss: 2.300084\n",
      "Batch 287/600, Batch Loss: 2.271072\n",
      "Batch 288/600, Batch Loss: 2.328866\n",
      "Batch 289/600, Batch Loss: 2.299842\n",
      "Batch 290/600, Batch Loss: 2.332402\n",
      "Batch 291/600, Batch Loss: 2.304294\n",
      "Batch 292/600, Batch Loss: 2.312541\n",
      "Batch 293/600, Batch Loss: 2.333237\n",
      "Batch 294/600, Batch Loss: 2.338642\n",
      "Batch 295/600, Batch Loss: 2.339548\n",
      "Batch 296/600, Batch Loss: 2.323513\n",
      "Batch 297/600, Batch Loss: 2.305465\n",
      "Batch 298/600, Batch Loss: 2.301458\n",
      "Batch 299/600, Batch Loss: 2.320413\n",
      "Batch 300/600, Batch Loss: 2.340710\n",
      "Batch 301/600, Batch Loss: 2.287705\n",
      "Batch 302/600, Batch Loss: 2.309332\n",
      "Batch 303/600, Batch Loss: 2.295236\n",
      "Batch 304/600, Batch Loss: 2.326405\n",
      "Batch 305/600, Batch Loss: 2.288464\n",
      "Batch 306/600, Batch Loss: 2.319319\n",
      "Batch 307/600, Batch Loss: 2.324545\n",
      "Batch 308/600, Batch Loss: 2.309577\n",
      "Batch 309/600, Batch Loss: 2.316073\n",
      "Batch 310/600, Batch Loss: 2.341496\n",
      "Batch 311/600, Batch Loss: 2.332406\n",
      "Batch 312/600, Batch Loss: 2.279133\n",
      "Batch 313/600, Batch Loss: 2.303994\n",
      "Batch 314/600, Batch Loss: 2.317408\n",
      "Batch 315/600, Batch Loss: 2.310755\n",
      "Batch 316/600, Batch Loss: 2.303805\n",
      "Batch 317/600, Batch Loss: 2.319798\n",
      "Batch 318/600, Batch Loss: 2.315972\n",
      "Batch 319/600, Batch Loss: 2.315737\n",
      "Batch 320/600, Batch Loss: 2.313940\n",
      "Batch 321/600, Batch Loss: 2.315848\n",
      "Batch 322/600, Batch Loss: 2.330070\n",
      "Batch 323/600, Batch Loss: 2.338986\n",
      "Batch 324/600, Batch Loss: 2.302062\n",
      "Batch 325/600, Batch Loss: 2.295515\n",
      "Batch 326/600, Batch Loss: 2.311958\n",
      "Batch 327/600, Batch Loss: 2.323102\n",
      "Batch 328/600, Batch Loss: 2.298743\n",
      "Batch 329/600, Batch Loss: 2.303992\n",
      "Batch 330/600, Batch Loss: 2.303513\n",
      "Batch 331/600, Batch Loss: 2.303932\n",
      "Batch 332/600, Batch Loss: 2.334548\n",
      "Batch 333/600, Batch Loss: 2.299325\n",
      "Batch 334/600, Batch Loss: 2.310617\n",
      "Batch 335/600, Batch Loss: 2.306014\n",
      "Batch 336/600, Batch Loss: 2.311459\n",
      "Batch 337/600, Batch Loss: 2.277624\n",
      "Batch 338/600, Batch Loss: 2.318619\n",
      "Batch 339/600, Batch Loss: 2.354482\n",
      "Batch 340/600, Batch Loss: 2.335495\n",
      "Batch 341/600, Batch Loss: 2.302326\n",
      "Batch 342/600, Batch Loss: 2.272229\n",
      "Batch 343/600, Batch Loss: 2.298433\n",
      "Batch 344/600, Batch Loss: 2.295788\n",
      "Batch 345/600, Batch Loss: 2.293318\n",
      "Batch 346/600, Batch Loss: 2.302326\n",
      "Batch 347/600, Batch Loss: 2.280341\n",
      "Batch 348/600, Batch Loss: 2.312742\n",
      "Batch 349/600, Batch Loss: 2.326784\n",
      "Batch 350/600, Batch Loss: 2.319898\n",
      "Batch 351/600, Batch Loss: 2.342434\n",
      "Batch 352/600, Batch Loss: 2.318505\n",
      "Batch 353/600, Batch Loss: 2.293718\n",
      "Batch 354/600, Batch Loss: 2.261426\n",
      "Batch 355/600, Batch Loss: 2.307804\n",
      "Batch 356/600, Batch Loss: 2.304727\n",
      "Batch 357/600, Batch Loss: 2.291881\n",
      "Batch 358/600, Batch Loss: 2.308592\n",
      "Batch 359/600, Batch Loss: 2.326906\n",
      "Batch 360/600, Batch Loss: 2.328251\n",
      "Batch 361/600, Batch Loss: 2.296766\n",
      "Batch 362/600, Batch Loss: 2.339751\n",
      "Batch 363/600, Batch Loss: 2.284016\n",
      "Batch 364/600, Batch Loss: 2.312497\n",
      "Batch 365/600, Batch Loss: 2.317422\n",
      "Batch 366/600, Batch Loss: 2.296526\n",
      "Batch 367/600, Batch Loss: 2.293005\n",
      "Batch 368/600, Batch Loss: 2.316974\n",
      "Batch 369/600, Batch Loss: 2.312425\n",
      "Batch 370/600, Batch Loss: 2.310268\n",
      "Batch 371/600, Batch Loss: 2.290615\n",
      "Batch 372/600, Batch Loss: 2.291555\n",
      "Batch 373/600, Batch Loss: 2.285274\n",
      "Batch 374/600, Batch Loss: 2.313589\n",
      "Batch 375/600, Batch Loss: 2.303404\n",
      "Batch 376/600, Batch Loss: 2.312399\n",
      "Batch 377/600, Batch Loss: 2.307306\n",
      "Batch 378/600, Batch Loss: 2.269103\n",
      "Batch 379/600, Batch Loss: 2.293623\n",
      "Batch 380/600, Batch Loss: 2.265445\n",
      "Batch 381/600, Batch Loss: 2.279388\n",
      "Batch 382/600, Batch Loss: 2.295848\n",
      "Batch 383/600, Batch Loss: 2.297777\n",
      "Batch 384/600, Batch Loss: 2.320199\n",
      "Batch 385/600, Batch Loss: 2.302302\n",
      "Batch 386/600, Batch Loss: 2.291157\n",
      "Batch 387/600, Batch Loss: 2.323403\n",
      "Batch 388/600, Batch Loss: 2.278724\n",
      "Batch 389/600, Batch Loss: 2.303650\n",
      "Batch 390/600, Batch Loss: 2.320717\n",
      "Batch 391/600, Batch Loss: 2.309220\n",
      "Batch 392/600, Batch Loss: 2.292080\n",
      "Batch 393/600, Batch Loss: 2.306259\n",
      "Batch 394/600, Batch Loss: 2.279892\n",
      "Batch 395/600, Batch Loss: 2.320843\n",
      "Batch 396/600, Batch Loss: 2.323887\n",
      "Batch 397/600, Batch Loss: 2.345095\n",
      "Batch 398/600, Batch Loss: 2.301359\n",
      "Batch 399/600, Batch Loss: 2.330525\n",
      "Batch 400/600, Batch Loss: 2.276468\n",
      "Batch 401/600, Batch Loss: 2.313102\n",
      "Batch 402/600, Batch Loss: 2.312774\n",
      "Batch 403/600, Batch Loss: 2.303827\n",
      "Batch 404/600, Batch Loss: 2.285758\n",
      "Batch 405/600, Batch Loss: 2.309009\n",
      "Batch 406/600, Batch Loss: 2.331608\n",
      "Batch 407/600, Batch Loss: 2.299492\n",
      "Batch 408/600, Batch Loss: 2.333337\n",
      "Batch 409/600, Batch Loss: 2.307198\n",
      "Batch 410/600, Batch Loss: 2.303277\n",
      "Batch 411/600, Batch Loss: 2.302210\n",
      "Batch 412/600, Batch Loss: 2.317643\n",
      "Batch 413/600, Batch Loss: 2.275193\n",
      "Batch 414/600, Batch Loss: 2.353978\n",
      "Batch 415/600, Batch Loss: 2.314146\n",
      "Batch 416/600, Batch Loss: 2.301210\n",
      "Batch 417/600, Batch Loss: 2.285506\n",
      "Batch 418/600, Batch Loss: 2.292703\n",
      "Batch 419/600, Batch Loss: 2.319032\n",
      "Batch 420/600, Batch Loss: 2.296039\n",
      "Batch 421/600, Batch Loss: 2.270184\n",
      "Batch 422/600, Batch Loss: 2.268463\n",
      "Batch 423/600, Batch Loss: 2.341006\n",
      "Batch 424/600, Batch Loss: 2.280720\n",
      "Batch 425/600, Batch Loss: 2.306347\n",
      "Batch 426/600, Batch Loss: 2.284452\n",
      "Batch 427/600, Batch Loss: 2.317443\n",
      "Batch 428/600, Batch Loss: 2.271376\n",
      "Batch 429/600, Batch Loss: 2.295728\n",
      "Batch 430/600, Batch Loss: 2.352881\n",
      "Batch 431/600, Batch Loss: 2.308061\n",
      "Batch 432/600, Batch Loss: 2.307989\n",
      "Batch 433/600, Batch Loss: 2.349904\n",
      "Batch 434/600, Batch Loss: 2.333561\n",
      "Batch 435/600, Batch Loss: 2.334154\n",
      "Batch 436/600, Batch Loss: 2.303775\n",
      "Batch 437/600, Batch Loss: 2.345755\n",
      "Batch 438/600, Batch Loss: 2.288453\n",
      "Batch 439/600, Batch Loss: 2.298602\n",
      "Batch 440/600, Batch Loss: 2.302243\n",
      "Batch 441/600, Batch Loss: 2.280781\n",
      "Batch 442/600, Batch Loss: 2.303135\n",
      "Batch 443/600, Batch Loss: 2.287162\n",
      "Batch 444/600, Batch Loss: 2.299693\n",
      "Batch 445/600, Batch Loss: 2.302556\n",
      "Batch 446/600, Batch Loss: 2.299281\n",
      "Batch 447/600, Batch Loss: 2.275484\n",
      "Batch 448/600, Batch Loss: 2.279866\n",
      "Batch 449/600, Batch Loss: 2.289975\n",
      "Batch 450/600, Batch Loss: 2.311424\n",
      "Batch 451/600, Batch Loss: 2.282959\n",
      "Batch 452/600, Batch Loss: 2.288620\n",
      "Batch 453/600, Batch Loss: 2.302473\n",
      "Batch 454/600, Batch Loss: 2.300312\n",
      "Batch 455/600, Batch Loss: 2.318230\n",
      "Batch 456/600, Batch Loss: 2.290571\n",
      "Batch 457/600, Batch Loss: 2.261794\n",
      "Batch 458/600, Batch Loss: 2.312023\n",
      "Batch 459/600, Batch Loss: 2.332386\n",
      "Batch 460/600, Batch Loss: 2.273032\n",
      "Batch 461/600, Batch Loss: 2.307916\n",
      "Batch 462/600, Batch Loss: 2.339175\n",
      "Batch 463/600, Batch Loss: 2.271286\n",
      "Batch 464/600, Batch Loss: 2.307246\n",
      "Batch 465/600, Batch Loss: 2.284863\n",
      "Batch 466/600, Batch Loss: 2.278346\n",
      "Batch 467/600, Batch Loss: 2.294986\n",
      "Batch 468/600, Batch Loss: 2.279245\n",
      "Batch 469/600, Batch Loss: 2.298812\n",
      "Batch 470/600, Batch Loss: 2.289690\n",
      "Batch 471/600, Batch Loss: 2.349530\n",
      "Batch 472/600, Batch Loss: 2.277781\n",
      "Batch 473/600, Batch Loss: 2.319017\n",
      "Batch 474/600, Batch Loss: 2.324090\n",
      "Batch 475/600, Batch Loss: 2.292582\n",
      "Batch 476/600, Batch Loss: 2.298788\n",
      "Batch 477/600, Batch Loss: 2.300118\n",
      "Batch 478/600, Batch Loss: 2.307779\n",
      "Batch 479/600, Batch Loss: 2.275149\n",
      "Batch 480/600, Batch Loss: 2.284017\n",
      "Batch 481/600, Batch Loss: 2.279774\n",
      "Batch 482/600, Batch Loss: 2.329089\n",
      "Batch 483/600, Batch Loss: 2.309664\n",
      "Batch 484/600, Batch Loss: 2.280968\n",
      "Batch 485/600, Batch Loss: 2.299450\n",
      "Batch 486/600, Batch Loss: 2.282042\n",
      "Batch 487/600, Batch Loss: 2.313279\n",
      "Batch 488/600, Batch Loss: 2.252914\n",
      "Batch 489/600, Batch Loss: 2.239267\n",
      "Batch 490/600, Batch Loss: 2.305443\n",
      "Batch 491/600, Batch Loss: 2.313021\n",
      "Batch 492/600, Batch Loss: 2.282275\n",
      "Batch 493/600, Batch Loss: 2.283198\n",
      "Batch 494/600, Batch Loss: 2.348913\n",
      "Batch 495/600, Batch Loss: 2.274306\n",
      "Batch 496/600, Batch Loss: 2.335180\n",
      "Batch 497/600, Batch Loss: 2.308984\n",
      "Batch 498/600, Batch Loss: 2.289402\n",
      "Batch 499/600, Batch Loss: 2.286224\n",
      "Batch 500/600, Batch Loss: 2.288050\n",
      "Batch 501/600, Batch Loss: 2.306306\n",
      "Batch 502/600, Batch Loss: 2.309088\n",
      "Batch 503/600, Batch Loss: 2.266998\n",
      "Batch 504/600, Batch Loss: 2.288607\n",
      "Batch 505/600, Batch Loss: 2.285084\n",
      "Batch 506/600, Batch Loss: 2.290815\n",
      "Batch 507/600, Batch Loss: 2.320518\n",
      "Batch 508/600, Batch Loss: 2.287092\n",
      "Batch 509/600, Batch Loss: 2.307062\n",
      "Batch 510/600, Batch Loss: 2.270807\n",
      "Batch 511/600, Batch Loss: 2.290242\n",
      "Batch 512/600, Batch Loss: 2.292862\n",
      "Batch 513/600, Batch Loss: 2.294588\n",
      "Batch 514/600, Batch Loss: 2.307537\n",
      "Batch 515/600, Batch Loss: 2.284714\n",
      "Batch 516/600, Batch Loss: 2.274539\n",
      "Batch 517/600, Batch Loss: 2.291201\n",
      "Batch 518/600, Batch Loss: 2.275698\n",
      "Batch 519/600, Batch Loss: 2.300119\n",
      "Batch 520/600, Batch Loss: 2.293178\n",
      "Batch 521/600, Batch Loss: 2.284561\n",
      "Batch 522/600, Batch Loss: 2.280609\n",
      "Batch 523/600, Batch Loss: 2.303572\n",
      "Batch 524/600, Batch Loss: 2.280930\n",
      "Batch 525/600, Batch Loss: 2.328172\n",
      "Batch 526/600, Batch Loss: 2.277544\n",
      "Batch 527/600, Batch Loss: 2.317051\n",
      "Batch 528/600, Batch Loss: 2.320050\n",
      "Batch 529/600, Batch Loss: 2.305554\n",
      "Batch 530/600, Batch Loss: 2.280484\n",
      "Batch 531/600, Batch Loss: 2.256703\n",
      "Batch 532/600, Batch Loss: 2.288932\n",
      "Batch 533/600, Batch Loss: 2.292746\n",
      "Batch 534/600, Batch Loss: 2.292123\n",
      "Batch 535/600, Batch Loss: 2.302244\n",
      "Batch 536/600, Batch Loss: 2.322260\n",
      "Batch 537/600, Batch Loss: 2.281019\n",
      "Batch 538/600, Batch Loss: 2.304579\n",
      "Batch 539/600, Batch Loss: 2.280653\n",
      "Batch 540/600, Batch Loss: 2.302161\n",
      "Batch 541/600, Batch Loss: 2.293791\n",
      "Batch 542/600, Batch Loss: 2.303182\n",
      "Batch 543/600, Batch Loss: 2.298604\n",
      "Batch 544/600, Batch Loss: 2.319760\n",
      "Batch 545/600, Batch Loss: 2.278750\n",
      "Batch 546/600, Batch Loss: 2.298195\n",
      "Batch 547/600, Batch Loss: 2.254046\n",
      "Batch 548/600, Batch Loss: 2.318419\n",
      "Batch 549/600, Batch Loss: 2.259192\n",
      "Batch 550/600, Batch Loss: 2.316637\n",
      "Batch 551/600, Batch Loss: 2.277399\n",
      "Batch 552/600, Batch Loss: 2.285363\n",
      "Batch 553/600, Batch Loss: 2.312662\n",
      "Batch 554/600, Batch Loss: 2.282852\n",
      "Batch 555/600, Batch Loss: 2.272845\n",
      "Batch 556/600, Batch Loss: 2.306585\n",
      "Batch 557/600, Batch Loss: 2.296562\n",
      "Batch 558/600, Batch Loss: 2.300856\n",
      "Batch 559/600, Batch Loss: 2.289226\n",
      "Batch 560/600, Batch Loss: 2.302243\n",
      "Batch 561/600, Batch Loss: 2.308301\n",
      "Batch 562/600, Batch Loss: 2.290661\n",
      "Batch 563/600, Batch Loss: 2.293737\n",
      "Batch 564/600, Batch Loss: 2.292455\n",
      "Batch 565/600, Batch Loss: 2.298165\n",
      "Batch 566/600, Batch Loss: 2.240924\n",
      "Batch 567/600, Batch Loss: 2.296158\n",
      "Batch 568/600, Batch Loss: 2.307704\n",
      "Batch 569/600, Batch Loss: 2.309472\n",
      "Batch 570/600, Batch Loss: 2.276626\n",
      "Batch 571/600, Batch Loss: 2.307616\n",
      "Batch 572/600, Batch Loss: 2.263096\n",
      "Batch 573/600, Batch Loss: 2.324725\n",
      "Batch 574/600, Batch Loss: 2.280570\n",
      "Batch 575/600, Batch Loss: 2.280456\n",
      "Batch 576/600, Batch Loss: 2.263354\n",
      "Batch 577/600, Batch Loss: 2.289463\n",
      "Batch 578/600, Batch Loss: 2.263579\n",
      "Batch 579/600, Batch Loss: 2.306557\n",
      "Batch 580/600, Batch Loss: 2.260282\n",
      "Batch 581/600, Batch Loss: 2.263833\n",
      "Batch 582/600, Batch Loss: 2.326873\n",
      "Batch 583/600, Batch Loss: 2.269316\n",
      "Batch 584/600, Batch Loss: 2.312951\n",
      "Batch 585/600, Batch Loss: 2.307555\n",
      "Batch 586/600, Batch Loss: 2.271523\n",
      "Batch 587/600, Batch Loss: 2.285681\n",
      "Batch 588/600, Batch Loss: 2.303403\n",
      "Batch 589/600, Batch Loss: 2.282113\n",
      "Batch 590/600, Batch Loss: 2.300090\n",
      "Batch 591/600, Batch Loss: 2.244124\n",
      "Batch 592/600, Batch Loss: 2.272636\n",
      "Batch 593/600, Batch Loss: 2.260660\n",
      "Batch 594/600, Batch Loss: 2.252486\n",
      "Batch 595/600, Batch Loss: 2.330760\n",
      "Batch 596/600, Batch Loss: 2.312997\n",
      "Batch 597/600, Batch Loss: 2.265589\n",
      "Batch 598/600, Batch Loss: 2.325286\n",
      "Batch 599/600, Batch Loss: 2.309741\n",
      "Batch 600/600, Batch Loss: 2.282798\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_dim = 784        # Number of input features\n",
    "    hidden1 = 512          # Number of neurons in first hidden layer\n",
    "    hidden2 = 256          # Number of neurons in second hidden layer\n",
    "    num_classes = 10       # Number of output classes\n",
    "\n",
    "    layer_sizes = [input_dim, hidden1, hidden2, num_classes]\n",
    "    activation_functions = ['relu', 'relu', 'linear']  # outputs logits\n",
    "\n",
    "    model = MLP(\n",
    "        layer_sizes=layer_sizes,\n",
    "        activation_functions=activation_functions\n",
    ")\n",
    "    training_data = X_train/255.0\n",
    "    training_targets = y_train\n",
    "    validation_data = X_test/255.0\n",
    "    validation_targets = y_test\n",
    "    \n",
    "    epochs = 15\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    model.train(\n",
    "        training_data=training_data,\n",
    "        training_targets=training_targets,\n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        validation_data=validation_data,\n",
    "        validation_targets=validation_targets,\n",
    "        batch_size=100\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10000):\n",
    "#     logits = model.forward(validation_data[i])\n",
    "#     probabilities = model.softmax(logits)\n",
    "#     predicted_class = np.argmax(probabilities)\n",
    "#     true_class = np.max(validation_targets[i])\n",
    "#     print(f\"Sample {i}: Predicted={predicted_class}, Actual={true_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72.39%\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.compute_accuracy(validation_data, validation_targets)\n",
    "print(f\"{accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
